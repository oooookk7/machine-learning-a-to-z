{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and sample dataset\n",
    "\n",
    "- Episode starts with taxi at a random square and passenger at random location, and ends with the passenger being dropped off at a specified destination.\n",
    "- 4 destinations: R(ed), G(reen), Y(ellow), and B(lue)\n",
    "\n",
    "There are `500` discrete states as `25` (taxi positions) × `5` (possible passenger locations) × `4` (destination).\n",
    "\n",
    "| Location index | Description |\n",
    "| -- | --- |\n",
    "| `0` | R(ed) |\n",
    "| `1` | G(reen) |\n",
    "| `2` | Y(ellow) |\n",
    "| `3` | B(lue) |\n",
    "| `4` | In taxi |\n",
    "\n",
    "There are `6` discrete deterministic actions:\n",
    "\n",
    "| Action index | Description |\n",
    "| -- | -- |\n",
    "| `0` | move south |\n",
    "| `1` | move north |\n",
    "| `2` | move east |\n",
    "| `3` | move west |\n",
    "| `4` | pickup passenger |\n",
    "| `5` | drop off passenger |\n",
    "\n",
    "The reward functions acts like this:\n",
    "\n",
    "| Reward value | Description |\n",
    "| -- | -- |\n",
    "| `-1` | Per step reward |\n",
    "| `+20` | Delivering passenger |\n",
    "| `-10` | Executing \"pickup\" or \"drop-off\" actions illegally |\n",
    "\n",
    "### Rendering\n",
    "\n",
    "These are the color indications,\n",
    "\n",
    "| Color | Description |\n",
    "| -- | -- |\n",
    "| Blue | Passenger |\n",
    "| Magenta | Destination |\n",
    "| Yellow | Empty taxi |\n",
    "| Green | Full taxi |\n",
    "\n",
    "These are the letter indications,\n",
    "\n",
    "| Letter | Description |\n",
    "| -- | -- |\n",
    "| R | R(ed) destination |\n",
    "| G | G(reen) destination |\n",
    "| Y | Y(ellow) destination |\n",
    "| B | B(lue) destination |\n",
    "\n",
    "The block represent the taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "streets = gym.make(\"Taxi-v3\").env\n",
    "streets.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find optimized policy and state-action function from Q-Learning\n",
    "\n",
    "Credits to [angps95@kaggle](https://www.kaggle.com/angps95/intro-to-reinforcement-learning-with-openai-gym/).\n",
    "\n",
    "This uses the ε-greedy policy for choosing the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed with 100000 episodes\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "learning_rate = 0.2\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.1\n",
    "\n",
    "no_of_states = streets.observation_space.n\n",
    "no_of_actions = streets.action_space.n\n",
    "no_of_episodes = 100000\n",
    "\n",
    "Q = np.zeros([no_of_states, no_of_actions])\n",
    "policy = np.ones([no_of_states, no_of_actions]) / no_of_actions\n",
    "\n",
    "\n",
    "def next_action(state):\n",
    "    # Exploration-vs-exploitation using ε-greedy algorithm.\n",
    "    tau = random.uniform(0, 1)\n",
    "    \n",
    "    if tau < epsilon:\n",
    "        return streets.action_space.sample()\n",
    "    \n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "\n",
    "# Run through all episodes to tabulate the state-action matrix.\n",
    "for i in range(1, no_of_episodes + 1):\n",
    "    state = streets.reset()\n",
    "    is_terminal_state = False\n",
    "\n",
    "    while not is_terminal_state:\n",
    "        action = next_action(state)\n",
    "        next_state, reward, is_terminal_state, _ = streets.step(action)\n",
    "\n",
    "        Q[state, action] = (1 - learning_rate) * Q[state, action] + learning_rate * (reward + discount_factor * np.max(Q[next_state]))\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "# Set the policy accordingly to be favourable to maximized reward action.\n",
    "for state in range(no_of_states):\n",
    "    policy[state] = np.eye(no_of_actions)[np.argmax(Q[state])]\n",
    "\n",
    "print(f'Completed with {no_of_episodes} episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the amount of steps taken upon using model algorithm\n",
    "\n",
    "Reset the environment after having pre-learnt it and find the amount of steps taken to reach goal.\n",
    "\n",
    "Display the min, max and avg steps after it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_steps():\n",
    "    current_state = streets.reset()\n",
    "    reward = 0\n",
    "    no_of_steps = 0\n",
    "\n",
    "    while reward != 20:\n",
    "        state, reward, _, _ = streets.step(np.argmax(policy[current_state]))  \n",
    "        current_state = state\n",
    "        no_of_steps += 1\n",
    "    \n",
    "    return no_of_steps\n",
    "\n",
    "episode_dist = np.array([episode_steps() for i in range(10000)])\n",
    "\n",
    "print(f'Min steps={np.min(episode_dist)}, Avg steps={np.round(np.average(episode_dist), 1)}, Max steps={np.max(episode_dist)}')\n",
    "streets.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display episode steps distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "bins = [i + 1 for i in range(np.min(episode_dist), np.max(episode_dist))]\n",
    "\n",
    "ax.hist(episode_dist, bins=bins)\n",
    "ax.set_title(\"Episode Steps Distribution\")\n",
    "ax.set_xticks(bins)\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('No. of episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
