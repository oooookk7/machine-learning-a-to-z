{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Create the RNN Basic Unit\n",
    "\n",
    "Here's the basic idea of the cell. The RNN is basically a repetition of the cell built over the time steps (e.g. `10` time steps).\n",
    "\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn_step_forward.png\" width=\"500\" height=\"auto\" />\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn.png\" width=\"800\" height=\"auto\" />\n",
    "\n",
    "(Source: [Fisseha Berhane, n.d.](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html))\n",
    "\n",
    "Basic idea of backward,\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn_cell_backprop.png\" width=\"700\" height=\"auto\" />\n",
    "\n",
    "(Source: [Fisseha Berhane, n.d.](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html))\n",
    "\n",
    "Credits: [Fisseha Berhane](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html) _(course)_, and [@brunoklein99](https://github.com/brunoklein99/deep-learning-notes/blob/master/rnn_utils.py) _(for some code)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "\n",
    "\n",
    "class BasicUnit:\n",
    "    \n",
    "    def __init__(self, x_size=3, y_size=2, a_size=5):\n",
    "        # Weight for multiplying current input\n",
    "        self.ax_weights = np.random.randn(a_size, x_size)\n",
    "        # Weight for multiplying past input (hidden-state)\n",
    "        self.aa_weights = np.random.randn(a_size, a_size)\n",
    "        # Weight for relating the hidden-state to output\n",
    "        self.ya_weights = np.random.randn(y_size, a_size)\n",
    "        # Bias for activation function\n",
    "        self.a_bias = np.random.randn(a_size, 1)\n",
    "        # Bias for relating the hidden-state to output\n",
    "        self.y_bias = np.random.randn(y_size, 1)\n",
    "\n",
    "    def cell_forward(self, x_values, a_prev):\n",
    "        a_next = np.tanh(np.dot(self.aa_weights, a_prev) + np.dot(self.ax_weights, x_values) + self.a_bias)\n",
    "        y_pred = softmax(np.dot(self.ya_weights, a_next) + self.y_bias)\n",
    "\n",
    "        return dict(a_prev=a_prev, a_next=a_next, y_pred=y_pred)\n",
    "\n",
    "    def cell_backward(self, x_values, hs, a_next_d):\n",
    "        # Gradient of tanh respect to a_next derivative\n",
    "        tanh_d = (1 - hs['a_next'] ** 2) * a_next_d\n",
    "\n",
    "        # Gradient of loss respect to W_ax\n",
    "        x_values_d = np.dot(self.ax_weights.T, tanh_d)\n",
    "        ax_weights_d = np.dot(tanh_d, x_values.T)\n",
    "\n",
    "        # Gradient with respect to W_aa\n",
    "        a_prev_d = np.dot(self.aa_weights.T, tanh_d)\n",
    "        aa_weights_d = np.dot(tanh_d, hs['a_prev'].T)\n",
    "\n",
    "        # Gradient with respect to b_a\n",
    "        a_bias_d = np.sum(tanh_d, 1, keepdims=True)\n",
    "\n",
    "        return dict(x_values_d=x_values_d, \n",
    "                    a_prev_d=a_prev_d, \n",
    "                    ax_weights_d=ax_weights_d, \n",
    "                    aa_weights_d=aa_weights_d, \n",
    "                    a_bias_d=a_bias_d)\n",
    "\n",
    "    def forward_pass(self, x_time_values, a_init=None):\n",
    "        y_size, a_size = self.ya_weights.shape\n",
    "        _, input_size, no_of_time_steps = x_time_values.shape\n",
    "\n",
    "        a = np.zeros((a_size, input_size, no_of_time_steps))\n",
    "        y_pred = np.zeros((y_size, input_size, no_of_time_steps))\n",
    "        hss = []\n",
    "\n",
    "        a_next = a_init if a_init is not None else np.zeros((a_size, input_size))\n",
    "\n",
    "        for t in range(no_of_time_steps):\n",
    "            cell = self.cell_forward(x_time_values[:,:,t], a_next)\n",
    "            a_next = cell['a_next']\n",
    "            a[:,:,t] = cell['a_next']\n",
    "            y_pred[:,:,t] = cell['y_pred']\n",
    "            hss.append(cell)\n",
    "\n",
    "        return dict(a=a, y_pred=y_pred, hss=hss)\n",
    "\n",
    "    def backward_pass(self, x_time_values, hss, a_init_d=None):\n",
    "        _, input_size, no_of_time_steps = x_time_values.shape\n",
    "        a_size = self.ya_weights.shape[1]\n",
    "\n",
    "        x_time_values_d = np.zeros(x_time_values.shape)\n",
    "        ax_weights_d_cum = np.zeros(self.ax_weights.shape)\n",
    "        aa_weights_d_cum = np.zeros(self.aa_weights.shape)\n",
    "        a_bias_d_cum = np.zeros(self.a_bias.shape)\n",
    "\n",
    "        a_d = a_init_d if a_init_d is not None else np.zeros((a_size, input_size))\n",
    "        a_prev_d = np.zeros((a_size, input_size))\n",
    "\n",
    "        for t in reversed(range(no_of_time_steps)):\n",
    "            cell_gradient = self.cell_backward(x_time_values[:,:,t], hss[t], a_prev_d + a_d[:,:, t])\n",
    "\n",
    "            x_time_values_d[:,:,t] = cell_gradient['x_values_d']\n",
    "            a_prev_d = cell_gradient['a_prev_d']\n",
    "\n",
    "            ax_weights_d_cum += cell_gradient['ax_weights_d']\n",
    "            aa_weights_d_cum += cell_gradient['aa_weights_d']\n",
    "            a_bias_d_cum += cell_gradient['a_bias_d']\n",
    "\n",
    "        return dict(x_values_d=x_time_values_d, \n",
    "                    a_prev_d=a_prev_d,\n",
    "                    ax_weights_d=ax_weights_d_cum, \n",
    "                    aa_weights_d=aa_weights_d_cum, \n",
    "                    a_bias_d=a_bias_d_cum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Preview pass-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward pass (peak @ [0][0]) ===\n",
      "Value at time 1:\n",
      "    a(t-1) = 0.0, \n",
      "    a(t) = -0.9704501385758645, \n",
      "    y_pred = 0.026321497913298138\n",
      "\n",
      "Value at time 2:\n",
      "    a(t-1) = -0.9704501385758645, \n",
      "    a(t) = 0.9999597278867156, \n",
      "    y_pred = 0.9882791423021918\n",
      "\n",
      "Value at time 3:\n",
      "    a(t-1) = 0.9999597278867156, \n",
      "    a(t) = 0.998689763217528, \n",
      "    y_pred = 0.8845671129063392\n",
      "\n",
      "Value at time 4:\n",
      "    a(t-1) = 0.998689763217528, \n",
      "    a(t) = 0.9998818752509105, \n",
      "    y_pred = 0.9865400974634416\n",
      "\n",
      "Value at time 5:\n",
      "    a(t-1) = 0.9998818752509105, \n",
      "    a(t) = 0.9185594012804403, \n",
      "    y_pred = 0.8291876750233819\n",
      "\n",
      "Values for weights and biases:\n",
      "    W_ax = -0.31011677351806,\n",
      "    W_aa = -0.17470315974250095,\n",
      "    W_ya = 1.1603385699937696,\n",
      "    b_a = -0.40087819178892664,\n",
      "    b_y = -1.7606885603987834\n",
      "\n",
      "=== Backward pass (peak @ [0][0]) ===\n",
      "Derivative at time 5:\n",
      "    x(t) = -0.1528142638110492\n",
      "\n",
      "Derivative at time 4:\n",
      "    x(t) = -0.05833336493553153\n",
      "\n",
      "Derivative at time 3:\n",
      "    x(t) = 0.7024169887436428\n",
      "\n",
      "Derivative at time 2:\n",
      "    x(t) = 0.5030053219744218\n",
      "\n",
      "Derivative at time 1:\n",
      "    x(t) = 0.0011642062948683893\n",
      "\n",
      "Derivatives for weights and biases:\n",
      "    W_ax = -1.347736683521624,\n",
      "    W_aa = -0.898450276092201,\n",
      "    b_a = -2.4126680408506846\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "no_of_time_steps = 5\n",
    "x_train = np.random.randn(3, 10, no_of_time_steps)\n",
    "\n",
    "unit = BasicUnit(x_size=x_train.shape[0], y_size=2, a_size=5)\n",
    "\n",
    "print('=== Forward pass (peak @ [0][0]) ===')\n",
    "\n",
    "fp_result = unit.forward_pass(x_train)\n",
    "\n",
    "for t in range(x_train.shape[2]):\n",
    "    print(f\"\"\"Value at time {t+1}:\n",
    "    a(t-1) = {fp_result['hss'][t]['a_prev'][0][0]}, \n",
    "    a(t) = {fp_result['a'][:,:,t][0][0]}, \n",
    "    y_pred = {fp_result['y_pred'][:,:,t][0][0]}\n",
    "\"\"\")\n",
    "    \n",
    "print(f\"\"\"Values for weights and biases:\n",
    "    W_ax = {unit.ax_weights[0][0]},\n",
    "    W_aa = {unit.aa_weights[0][0]},\n",
    "    W_ya = {unit.ya_weights[0][0]},\n",
    "    b_a = {unit.a_bias[0][0]},\n",
    "    b_y = {unit.y_bias[0][0]}\n",
    "\"\"\")\n",
    "\n",
    "print('=== Backward pass (peak @ [0][0]) ===')\n",
    "\n",
    "bp = unit.backward_pass(x_train, fp_result['hss'], a_init_d=np.random.randn(*fp_result['a'].shape))\n",
    "\n",
    "for t in reversed(range(x_train.shape[2])):\n",
    "    print(f\"\"\"Derivative at time {t+1}:\n",
    "    x(t) = {bp['x_values_d'][:,:,t][0][0]}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"Derivatives for weights and biases:\n",
    "    W_ax = {bp['ax_weights_d'][0][0]},\n",
    "    W_aa = {bp['aa_weights_d'][0][0]},\n",
    "    b_a = {bp['a_bias_d'][0][0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Create the LSTM Unit\n",
    "\n",
    "Here's the basic idea of the cell. For illustration sake, we assume our task as reading words in a piece of text.\n",
    "\n",
    "- **Forget gate**: Keep track of grammatical structures (e.g. if subject is singular or plural). Gets rid of previously stored memory when subject has changed. Values: `0` (forget) - `1` (keep).\n",
    "\n",
    "- **Update gate**: Find a way to update to reflect that new subject is plural. Values: `0` - `1`.\n",
    "\n",
    "- **Output gate**: Decide which outputs to use.\n",
    "\n",
    "<img src=\"https://datascience-enthusiast.com/figures/LSTM.png\" width=\"800\" height=\"auto\" />\n",
    "<img src=\"https://datascience-enthusiast.com/figures/LSTM_rnn.png\" width=\"1000\" height=\"auto\" />\n",
    "\n",
    "(Source: [Fisseha Berhane, n.d.](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class LSTMUnit:\n",
    "    \n",
    "    def __init__(self, x_size=3, y_size=2, a_size=5):\n",
    "        # Weight for forget gate\n",
    "        self.f_weights = np.random.randn(a_size, a_size + x_size)\n",
    "        # Weight for update gate\n",
    "        self.i_weights = np.random.randn(a_size, a_size + x_size)\n",
    "        # Weight for first \"tanh\"\n",
    "        self.c_weights = np.random.randn(a_size, a_size + x_size)\n",
    "        # Weight for output gate\n",
    "        self.o_weights = np.random.randn(a_size, a_size + x_size)\n",
    "        # Weight for relating the hidden-state to output\n",
    "        self.y_weights = np.random.randn(y_size, a_size)\n",
    "        # Bias for forget gate\n",
    "        self.f_bias = np.random.randn(a_size, 1)\n",
    "        # Bias for update gate\n",
    "        self.i_bias = np.random.randn(a_size, 1)\n",
    "        # Bias for first \"tanh\"\n",
    "        self.c_bias = np.random.randn(a_size, 1)\n",
    "        # Bias for output gate\n",
    "        self.o_bias = np.random.randn(a_size, 1)\n",
    "        # Bias for relating the hidden-state to output\n",
    "        self.y_bias = np.random.randn(y_size, 1)\n",
    "\n",
    "    def cell_forward(self, x_values, a_prev, c_prev):\n",
    "        x_size, input_size = x_values.shape\n",
    "        y_size, a_size = self.y_weights.shape\n",
    "        \n",
    "        # Concatenate a_prev with x_values\n",
    "        concat = np.zeros((a_size + x_size, input_size))\n",
    "        concat[:a_size,:] = a_prev\n",
    "        concat[a_size:,:] = x_values\n",
    "        \n",
    "        forget_v = sigmoid(np.dot(self.f_weights, concat) + self.f_bias)\n",
    "        update_v = sigmoid(np.dot(self.i_weights, concat) + self.i_bias)\n",
    "        tanh_v = np.tanh(np.dot(self.c_weights, concat) + self.c_bias)\n",
    "        output_v = sigmoid(np.dot(self.o_weights, concat) + self.o_bias)\n",
    "\n",
    "        c_next = forget_v * c_prev + update_v * tanh_v\n",
    "        a_next = output_v * np.tanh(c_next)\n",
    "\n",
    "        y_pred = softmax(np.dot(self.y_weights, a_next) + self.y_bias)\n",
    "\n",
    "        return dict(a_prev=a_prev, \n",
    "                    a_next=a_next, \n",
    "                    c_prev=c_prev, \n",
    "                    c_next=c_next,\n",
    "                    forget=forget_v,\n",
    "                    update=update_v,\n",
    "                    tanh=tanh_v,\n",
    "                    output=output_v,\n",
    "                    y_pred=y_pred)\n",
    "\n",
    "    def cell_backward(self, x_values, hs, a_next_d, c_next_d):\n",
    "        a_size = hs['a_next'].shape[0]\n",
    "\n",
    "        output_v_d = a_next_d * np.tanh(hs['c_next']) * \\\n",
    "                     hs['output'] * (1 - hs['output'])\n",
    "        \n",
    "        tanh_v_d = (c_next_d * hs['update'] + hs['output'] * (1 - np.square(np.tanh(hs['c_next']))) * \\\n",
    "                     hs['update'] * a_next_d) * \\\n",
    "                   (1 - np.square(hs['tanh']))\n",
    "        \n",
    "        update_v_d = (c_next_d * hs['tanh'] + hs['output'] * (1 - np.square(np.tanh(hs['c_next']))) * \\\n",
    "                       hs['tanh'] * a_next_d) * \\\n",
    "                     hs['update'] * (1 - hs['update'])\n",
    "        \n",
    "        forget_v_d = (c_next_d * hs['c_prev'] + hs['output'] * (1 - np.square(np.tanh(hs['c_next']))) * \\\n",
    "                       hs['c_prev'] * a_next_d) * \\\n",
    "                     hs['forget'] * (1 - hs['forget'])\n",
    "\n",
    "        concat = np.concatenate((hs['a_prev'], x_values), axis=0)\n",
    "\n",
    "        f_weights_d = np.dot(forget_v_d, concat.T)\n",
    "        i_weights_d = np.dot(update_v_d, concat.T)\n",
    "        c_weights_d = np.dot(tanh_v_d, concat.T)\n",
    "        o_weights_d = np.dot(output_v_d, concat.T)\n",
    "\n",
    "        f_bias_d = np.sum(forget_v_d, axis=1 ,keepdims=True)\n",
    "        i_bias_d = np.sum(update_v_d, axis=1, keepdims=True)\n",
    "        c_bias_d = np.sum(tanh_v_d, axis=1,  keepdims=True)\n",
    "        o_bias_d = np.sum(output_v_d, axis=1, keepdims=True)\n",
    "\n",
    "        a_prev_d = np.dot(self.f_weights[:, :a_size].T, forget_v_d) + \\\n",
    "                   np.dot(self.i_weights[:, :a_size].T, update_v_d) + \\\n",
    "                   np.dot(self.c_weights[:, :a_size].T, tanh_v_d) + \\\n",
    "                   np.dot(self.o_weights[:, :a_size].T, output_v_d)\n",
    "\n",
    "        c_prev_d = c_next_d * hs['forget'] + hs['output'] * \\\n",
    "                   (1 - np.square(np.tanh(hs['c_next']))) * hs['forget'] * a_next_d\n",
    "\n",
    "        x_values_d = np.dot(self.f_weights[:, a_size:].T, forget_v_d) + \\\n",
    "                     np.dot(self.i_weights[:, a_size:].T, update_v_d) + \\\n",
    "                     np.dot(self.c_weights[:, a_size:].T, tanh_v_d) + \\\n",
    "                     np.dot(self.o_weights[:, a_size:].T, output_v_d)\n",
    "\n",
    "        return dict(output_d=output_v_d,\n",
    "                    tanh_d=tanh_v_d,\n",
    "                    update_d=update_v_d,\n",
    "                    forget_d=forget_v_d,\n",
    "                    f_weights_d=f_weights_d,\n",
    "                    i_weights_d=i_weights_d,\n",
    "                    c_weights_d=c_weights_d,\n",
    "                    o_weights_d=o_weights_d,\n",
    "                    f_bias_d=f_bias_d,\n",
    "                    i_bias_d=i_bias_d,\n",
    "                    c_bias_d=c_bias_d,\n",
    "                    o_bias_d=o_bias_d,\n",
    "                    a_prev_d=a_prev_d,\n",
    "                    c_prev_d=c_prev_d,\n",
    "                    x_values_d=x_values_d)\n",
    "\n",
    "    def forward_pass(self, x_time_values, a_init=None):\n",
    "        y_size, a_size = self.y_weights.shape\n",
    "        _, input_size, no_of_time_steps = x_time_values.shape\n",
    "\n",
    "        a = np.zeros((a_size, input_size, no_of_time_steps))\n",
    "        c = a\n",
    "        y_pred = np.zeros((y_size, input_size, no_of_time_steps))\n",
    "        hss = []\n",
    "    \n",
    "        a_next = a_init if a_init is not None else np.zeros((a_size, input_size))\n",
    "        c_next = np.zeros(a_next.shape)\n",
    "\n",
    "        for t in range(no_of_time_steps):\n",
    "            uc = self.cell_forward(x_time_values[:,:,t], a_next, c_next)\n",
    "            a_next = uc['a_next']\n",
    "            c_next = uc['c_next']\n",
    "            a[:,:,t] = uc['a_next']\n",
    "            c[:,:,t] = uc['c_next']\n",
    "            y_pred[:,:,t] = uc['y_pred']\n",
    "            hss.append(uc)\n",
    "\n",
    "        return dict(a=a, c=c, y_pred=y_pred, hss=hss)\n",
    "\n",
    "    def backward_pass(self, x_time_values, hss, a_init_d=None, c_init_d=None):\n",
    "        _, input_size, no_of_time_steps = x_time_values.shape\n",
    "        a_size = self.y_weights.shape[1]\n",
    "\n",
    "        x_time_values_d = np.zeros(x_time_values.shape)\n",
    "\n",
    "        f_weights_d_cum = np.zeros(self.f_weights.shape)\n",
    "        i_weights_d_cum = np.zeros(self.i_weights.shape)\n",
    "        c_weights_d_cum = np.zeros(self.c_weights.shape)\n",
    "        o_weights_d_cum = np.zeros(self.o_weights.shape)\n",
    "\n",
    "        f_bias_d_cum = np.zeros(self.f_bias.shape)\n",
    "        i_bias_d_cum = np.zeros(self.i_bias.shape)\n",
    "        c_bias_d_cum = np.zeros(self.c_bias.shape)\n",
    "        o_bias_d_cum = np.zeros(self.o_bias.shape)\n",
    "\n",
    "        a_d = a_init_d if a_init_d is not None else np.zeros((a_size, input_size))\n",
    "        c_d = c_init_d if c_init_d is not None else np.zeros(a_d.shape)\n",
    "\n",
    "        a_prev_d = np.zeros((a_size, input_size))\n",
    "        c_prev_d = np.zeros(a_prev_d.shape)\n",
    "\n",
    "        for t in reversed(range(no_of_time_steps)):\n",
    "            cell_gradient = self.cell_backward(x_time_values[:,:,t], hss[t], a_prev_d + a_d[:,:,t], c_prev_d + c_d[:,:,t])\n",
    "            \n",
    "            x_time_values_d[:,:,t] = cell_gradient['x_values_d']\n",
    "            f_weights_d_cum += cell_gradient['f_weights_d']\n",
    "            i_weights_d_cum += cell_gradient['i_weights_d']\n",
    "            c_weights_d_cum += cell_gradient['c_weights_d']\n",
    "            o_weights_d_cum += cell_gradient['o_weights_d']\n",
    "            \n",
    "            \n",
    "            f_bias_d_cum += cell_gradient['f_bias_d']\n",
    "            i_bias_d_cum += cell_gradient['i_bias_d']\n",
    "            c_bias_d_cum += cell_gradient['c_bias_d']\n",
    "            o_bias_d_cum += cell_gradient['o_bias_d']\n",
    "\n",
    "            a_prev_d = cell_gradient['a_prev_d']\n",
    "            c_prev_d = cell_gradient['c_prev_d']\n",
    "        \n",
    "        return dict(f_weights_d=f_weights_d_cum,\n",
    "                    i_weights_d=i_weights_d_cum,\n",
    "                    c_weights_d=c_weights_d_cum,\n",
    "                    o_weights_d=o_weights_d_cum,\n",
    "                    f_bias_d=f_bias_d_cum,\n",
    "                    i_bias_d=i_bias_d_cum,\n",
    "                    c_bias_d=c_bias_d_cum,\n",
    "                    o_bias_d=o_bias_d_cum,\n",
    "                    a_prev_d=a_prev_d,\n",
    "                    c_prev_d=c_prev_d,\n",
    "                    x_values_d=x_time_values_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Preview pass-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward pass (peak @ [0][0]) ===\n",
      "Value at time 1:\n",
      "    a(t-1) = 0.0, \n",
      "    a(t) = -0.14353153875994148, \n",
      "    c(t-1) = 0.0, \n",
      "    c(t) = -0.14353153875994148,\n",
      "    y_pred = 0.17163846940788854,\n",
      "\n",
      "    forget gate=0.42888833696800316,\n",
      "    update gate=0.15518207490544242,\n",
      "    tanh=-0.9249234413665368,\n",
      "    output gate=0.04692662136616572\n",
      "\n",
      "Value at time 2:\n",
      "    a(t-1) = -0.006689575154730827, \n",
      "    a(t) = 0.354065631568152, \n",
      "    c(t-1) = -0.14353153875994148, \n",
      "    c(t) = 0.354065631568152,\n",
      "    y_pred = 0.19492958598119411,\n",
      "\n",
      "    forget gate=0.5484609360623877,\n",
      "    update gate=0.8762617752533526,\n",
      "    tanh=0.4939015781508593,\n",
      "    output gate=0.7998679960399565\n",
      "\n",
      "Value at time 3:\n",
      "    a(t-1) = 0.27193609117543166, \n",
      "    a(t) = 1.0045654211001764, \n",
      "    c(t-1) = 0.354065631568152, \n",
      "    c(t) = 1.0045654211001764,\n",
      "    y_pred = 0.21383870432464613,\n",
      "\n",
      "    forget gate=0.5077195324369859,\n",
      "    update gate=0.8434860404996112,\n",
      "    tanh=0.9778459210775385,\n",
      "    output gate=0.8975712697590178\n",
      "\n",
      "Value at time 4:\n",
      "    a(t-1) = 0.6853000256621612, \n",
      "    a(t) = 1.3730294561068748, \n",
      "    c(t-1) = 1.0045654211001764, \n",
      "    c(t) = 1.3730294561068748,\n",
      "    y_pred = 0.24386146360517658,\n",
      "\n",
      "    forget gate=0.4350326895113357,\n",
      "    update gate=0.9738858436574737,\n",
      "    tanh=0.9611092154911564,\n",
      "    output gate=0.8940073042279727\n",
      "\n",
      "Value at time 5:\n",
      "    a(t-1) = 0.7861728328568451, \n",
      "    a(t) = 0.8588177198092162, \n",
      "    c(t-1) = 1.3730294561068748, \n",
      "    c(t) = 0.8588177198092162,\n",
      "    y_pred = 0.21166914070351162,\n",
      "\n",
      "    forget gate=0.4391087799481217,\n",
      "    update gate=0.5030915907881999,\n",
      "    tanh=0.508671651824588,\n",
      "    output gate=0.145322162544974\n",
      "\n",
      "Values for weights and biases:\n",
      "    W_f = -0.31011677351806\n",
      "    W_o = 0.8360047194342688\n",
      "    W_i = 1.1603385699937696\n",
      "    W_c = 0.24266944108179458\n",
      "    W_y = 1.1234121620219353\n",
      "    b_f = -0.13597732610058932\n",
      "    b_o = -0.6180368476815788\n",
      "    b_i = 0.9561217041246964\n",
      "    b_c = 0.4381663472912375\n",
      "    b_y = -0.41163916318848254\n",
      "\n",
      "=== Backward pass (peak @ [0][0]) ===\n",
      "Derivative at time 5:\n",
      "    x(t) = 0.40531244600573335\n",
      "\n",
      "Derivative at time 4:\n",
      "    x(t) = -0.41590885549895085\n",
      "\n",
      "Derivative at time 3:\n",
      "    x(t) = -0.46770006529986574\n",
      "\n",
      "Derivative at time 2:\n",
      "    x(t) = -2.21585372389746\n",
      "\n",
      "Derivative at time 1:\n",
      "    x(t) = 0.4545145611645156\n",
      "\n",
      "Derivatives for weights and biases:\n",
      "    W_f = 0.4787191811060379,\n",
      "    W_c = 0.13622286312118848,\n",
      "    W_i = 0.007127185804943648,\n",
      "    W_o = 0.517601164523045,\n",
      "    b_f = -0.3279504096184549,\n",
      "    b_c = 3.077110591091556,\n",
      "    b_i = 0.49746848980879,\n",
      "    b_o = 0.49746848980879\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "no_of_time_steps = 5\n",
    "x_train = np.random.randn(3, 10, no_of_time_steps)\n",
    "\n",
    "unit = LSTMUnit(x_size=x_train.shape[0], y_size=2, a_size=5)\n",
    "\n",
    "print('=== Forward pass (peak @ [0][0]) ===')\n",
    "\n",
    "fp_result = unit.forward_pass(x_train)\n",
    "\n",
    "for t in range(x_train.shape[2]):\n",
    "    print(f\"\"\"Value at time {t+1}:\n",
    "    a(t-1) = {fp_result['hss'][t]['a_prev'][0][0]}, \n",
    "    a(t) = {fp_result['a'][:,:,t][0][0]}, \n",
    "    c(t-1) = {fp_result['hss'][t]['c_prev'][0][0]}, \n",
    "    c(t) = {fp_result['c'][:,:,t][0][0]},\n",
    "    y_pred = {fp_result['y_pred'][:,:,t][0][0]},\n",
    "\n",
    "    forget gate={fp_result['hss'][t]['forget'][0][0]},\n",
    "    update gate={fp_result['hss'][t]['update'][0][0]},\n",
    "    tanh={fp_result['hss'][t]['tanh'][0][0]},\n",
    "    output gate={fp_result['hss'][t]['output'][0][0]}\n",
    "\"\"\")\n",
    "    \n",
    "print(f\"\"\"Values for weights and biases:\n",
    "    W_f = {unit.f_weights[0][0]}\n",
    "    W_o = {unit.o_weights[0][0]}\n",
    "    W_i = {unit.i_weights[0][0]}\n",
    "    W_c = {unit.c_weights[0][0]}\n",
    "    W_y = {unit.y_weights[0][0]}\n",
    "    b_f = {unit.f_bias[0][0]}\n",
    "    b_o = {unit.o_bias[0][0]}\n",
    "    b_i = {unit.i_bias[0][0]}\n",
    "    b_c = {unit.c_bias[0][0]}\n",
    "    b_y = {unit.y_bias[0][0]}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print('=== Backward pass (peak @ [0][0]) ===')\n",
    "\n",
    "bp = unit.backward_pass(x_train, fp_result['hss'], a_init_d=np.random.randn(*fp_result['a'].shape), \n",
    "                                                   c_init_d=np.random.randn(*fp_result['c'].shape))\n",
    "\n",
    "for t in reversed(range(x_train.shape[2])):\n",
    "    print(f\"\"\"Derivative at time {t+1}:\n",
    "    x(t) = {bp['x_values_d'][:,:,t][0][0]}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"Derivatives for weights and biases:\n",
    "    W_f = {bp['f_weights_d'][0][0]},\n",
    "    W_c = {bp['c_weights_d'][0][0]},\n",
    "    W_i = {bp['i_weights_d'][0][0]},\n",
    "    W_o = {bp['o_weights_d'][0][0]},\n",
    "    b_f = {bp['f_bias_d'][0][0]},\n",
    "    b_c = {bp['c_bias_d'][0][0]},\n",
    "    b_i = {bp['o_bias_d'][0][0]},\n",
    "    b_o = {bp['o_bias_d'][0][0]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON0YxX/oky4tPbqCLnFjWD",
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
