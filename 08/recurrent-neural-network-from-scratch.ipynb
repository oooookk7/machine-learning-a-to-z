{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Create the RNN Basic Unit\n",
    "\n",
    "Here's the basic idea of the cell. The RNN is basically a repetition of the cell built over the time steps (e.g. `10` time steps, meaning `10` inputs features for `x_1, ..., x_10`).\n",
    "\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn_step_forward.png\" width=\"500\" height=\"auto\" />\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn.png\" width=\"800\" height=\"auto\" />\n",
    "\n",
    "(Source: [Fisseha Berhane, n.d.](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html))\n",
    "\n",
    "Basic idea of backward,\n",
    "<img src=\"https://datascience-enthusiast.com/figures/rnn_cell_backprop.png\" width=\"700\" height=\"auto\" />\n",
    "\n",
    "(Source: [Fisseha Berhane, n.d.](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html))\n",
    "\n",
    "Credits: [Fisseha Berhane](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html) _(course)_, and [@brunoklein99](https://github.com/brunoklein99/deep-learning-notes/blob/master/rnn_utils.py) _(for some code)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / e_z.sum(axis=0)\n",
    "\n",
    "\n",
    "class BasicUnit:\n",
    "    \n",
    "    def __init__(self, x_n=3, y_n=2, a_n=5):\n",
    "        # Weight for multiplying current input\n",
    "        self.ax_weights = np.random.randn(a_n, x_n)\n",
    "        # Weight for multiplying past input (hidden-state)\n",
    "        self.aa_weights = np.random.randn(a_n, a_n)\n",
    "        # Weight for relating the hidden-state to output\n",
    "        self.ya_weights = np.random.randn(y_n, a_n)\n",
    "        # Bias for activation function\n",
    "        self.a_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for relating the hidden-state to output\n",
    "        self.y_bias = np.random.randn(y_n, 1)\n",
    "\n",
    "    def cell_forward(self, x_values, a_prev):\n",
    "        a_next = np.tanh(np.dot(self.aa_weights, a_prev) + np.dot(self.ax_weights, x_values) + self.a_bias)\n",
    "        y_pred = softmax(np.dot(self.ya_weights, a_next) + self.y_bias)\n",
    "\n",
    "        return dict(a_prev=a_prev, a_next=a_next, y_pred=y_pred)\n",
    "\n",
    "    def cell_backward(self, x_values, hs, a_next_d):\n",
    "        # Gradient of tanh respect to a_next derivative\n",
    "        tanh_d = (1 - hs['a_next'] ** 2) * a_next_d\n",
    "\n",
    "        # Gradient of loss respect to W_ax\n",
    "        x_values_d = np.dot(self.ax_weights.T, tanh_d)\n",
    "        ax_weights_d = np.dot(tanh_d, x_values.T)\n",
    "\n",
    "        # Gradient with respect to W_aa\n",
    "        a_prev_d = np.dot(self.aa_weights.T, tanh_d)\n",
    "        aa_weights_d = np.dot(tanh_d, hs['a_prev'].T)\n",
    "\n",
    "        # Gradient with respect to b_a\n",
    "        a_bias_d = np.sum(tanh_d, 1, keepdims=True)\n",
    "\n",
    "        return dict(x_values_d=x_values_d, \n",
    "                    a_prev_d=a_prev_d, \n",
    "                    ax_weights_d=ax_weights_d, \n",
    "                    aa_weights_d=aa_weights_d, \n",
    "                    a_bias_d=a_bias_d)\n",
    "\n",
    "    def forward_pass(self, x_time_values, a_init=None):\n",
    "        y_n, a_n = self.ya_weights.shape\n",
    "        _, x_m, features_n = x_time_values.shape\n",
    "\n",
    "        a = np.zeros((a_n, x_m, features_n))\n",
    "        y_pred = np.zeros((y_n, x_m, features_n))\n",
    "        # History cache\n",
    "        hss = []\n",
    "\n",
    "        a_next = a_init if a_init is not None else np.zeros((a_n, x_m))\n",
    "\n",
    "        for t in range(features_n):\n",
    "            cell = self.cell_forward(x_time_values[:,:,t], a_next)\n",
    "            a_next = cell['a_next']\n",
    "            a[:,:,t] = cell['a_next']\n",
    "            y_pred[:,:,t] = cell['y_pred']\n",
    "            hss.append(cell)\n",
    "\n",
    "        return dict(a=a, y_pred=y_pred, hss=hss)\n",
    "\n",
    "    def backward_pass(self, x_time_values, hss, a_init_d=None):\n",
    "        _, x_m, features_n = x_time_values.shape\n",
    "        a_n = self.ya_weights.shape[1]\n",
    "\n",
    "        x_time_values_d = np.zeros(x_time_values.shape)\n",
    "        ax_weights_d_cum = np.zeros(self.ax_weights.shape)\n",
    "        aa_weights_d_cum = np.zeros(self.aa_weights.shape)\n",
    "        a_bias_d_cum = np.zeros(self.a_bias.shape)\n",
    "\n",
    "        a_d = a_init_d if a_init_d is not None else np.zeros((a_n, x_m, features_n))\n",
    "        a_prev_d = np.zeros((a_n, x_m))\n",
    "\n",
    "        for t in reversed(range(features_n)):\n",
    "            print(a_prev_d.shape, a_d[:,:,t].shape)\n",
    "            a_prev_d + a_d[:,:,t]\n",
    "            cell_gradient = self.cell_backward(x_time_values[:,:,t], hss[t], a_prev_d + a_d[:,:,t])\n",
    "\n",
    "            x_time_values_d[:,:,t] = cell_gradient['x_values_d']\n",
    "            a_prev_d = cell_gradient['a_prev_d']\n",
    "\n",
    "            ax_weights_d_cum += cell_gradient['ax_weights_d']\n",
    "            aa_weights_d_cum += cell_gradient['aa_weights_d']\n",
    "            a_bias_d_cum += cell_gradient['a_bias_d']\n",
    "\n",
    "        return dict(x_values_d=x_time_values_d, \n",
    "                    a_prev_d=a_prev_d,\n",
    "                    ax_weights_d=ax_weights_d_cum, \n",
    "                    aa_weights_d=aa_weights_d_cum, \n",
    "                    a_bias_d=a_bias_d_cum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Preview pass-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward pass (peak @ [0][0]) ===\n",
      "Value at time 1:\n",
      "    a(t-1) = 0.0, \n",
      "    a(t) = -0.6758264535817718, \n",
      "    y_pred = 0.00042255310469995687\n",
      "\n",
      "Value at time 2:\n",
      "    a(t-1) = -0.6758264535817718, \n",
      "    a(t) = 0.2430228309281321, \n",
      "    y_pred = 0.24597191206471836\n",
      "\n",
      "Value at time 3:\n",
      "    a(t-1) = 0.2430228309281321, \n",
      "    a(t) = -0.5729623577084761, \n",
      "    y_pred = 0.08878426145466899\n",
      "\n",
      "Value at time 4:\n",
      "    a(t-1) = -0.5729623577084761, \n",
      "    a(t) = -0.9860486609143284, \n",
      "    y_pred = 0.9965121118034168\n",
      "\n",
      "Values for weights and biases:\n",
      "    W_ax = -0.024616955875778355,\n",
      "    W_aa = -0.6235307296797916,\n",
      "    W_ya = -0.5170944579202279,\n",
      "    b_a = -0.646916688254908,\n",
      "    b_y = -0.22631424251360518\n",
      "\n",
      "=== Backward pass (peak @ [0][0]) ===\n",
      "(5, 10) (5, 10)\n",
      "(5, 10) (5, 10)\n",
      "(5, 10) (5, 10)\n",
      "(5, 10) (5, 10)\n",
      "Derivative at time 4:\n",
      "    x(t) = 0.10360058464145615\n",
      "\n",
      "Derivative at time 3:\n",
      "    x(t) = 1.5662417561397577\n",
      "\n",
      "Derivative at time 2:\n",
      "    x(t) = -0.06048178204429309\n",
      "\n",
      "Derivative at time 1:\n",
      "    x(t) = -0.042608395617184096\n",
      "\n",
      "Derivatives for weights and biases:\n",
      "    W_ax = 0.4783599522351609,\n",
      "    W_aa = -1.6900350723551276,\n",
      "    b_a = 4.768703772466121\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "no_of_features = 4\n",
    "x_train = np.random.randn(3, 10, no_of_features)\n",
    "\n",
    "unit = BasicUnit(x_n=x_train.shape[0], y_n=2, a_n=5)\n",
    "\n",
    "print('=== Forward pass (peak @ [0][0]) ===')\n",
    "\n",
    "fp_result = unit.forward_pass(x_train)\n",
    "\n",
    "for t in range(x_train.shape[2]):\n",
    "    print(f\"\"\"Value at time {t+1}:\n",
    "    a(t-1) = {fp_result['hss'][t]['a_prev'][0][0]}, \n",
    "    a(t) = {fp_result['a'][:,:,t][0][0]}, \n",
    "    y_pred = {fp_result['y_pred'][:,:,t][0][0]}\n",
    "\"\"\")\n",
    "    \n",
    "print(f\"\"\"Values for weights and biases:\n",
    "    W_ax = {unit.ax_weights[0][0]},\n",
    "    W_aa = {unit.aa_weights[0][0]},\n",
    "    W_ya = {unit.ya_weights[0][0]},\n",
    "    b_a = {unit.a_bias[0][0]},\n",
    "    b_y = {unit.y_bias[0][0]}\n",
    "\"\"\")\n",
    "\n",
    "print('=== Backward pass (peak @ [0][0]) ===')\n",
    "\n",
    "bp = unit.backward_pass(x_train, fp_result['hss'], a_init_d=np.random.randn(*fp_result['a'].shape))\n",
    "\n",
    "for t in reversed(range(x_train.shape[2])):\n",
    "    print(f\"\"\"Derivative at time {t+1}:\n",
    "    x(t) = {bp['x_values_d'][:,:,t][0][0]}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"Derivatives for weights and biases:\n",
    "    W_ax = {bp['ax_weights_d'][0][0]},\n",
    "    W_aa = {bp['aa_weights_d'][0][0]},\n",
    "    b_a = {bp['a_bias_d'][0][0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Create the LSTM Unit\n",
    "\n",
    "Here's the basic idea of the cell. For illustration sake, we assume our task as reading words in a piece of text.\n",
    "\n",
    "- **Forget gate**: Keep track of grammatical structures (e.g. if subject is singular or plural). Gets rid of previously stored memory when subject has changed. Values: `0` (forget) - `1` (keep).\n",
    "\n",
    "- **Update gate**: Find a way to update to reflect that new subject is plural. Values: `0` - `1`.\n",
    "\n",
    "- **Output gate**: Decide which outputs to use.\n",
    "\n",
    "<img src=\"https://datascience-enthusiast.com/figures/LSTM.png\" width=\"800\" height=\"auto\" />\n",
    "<img src=\"https://datascience-enthusiast.com/figures/LSTM_rnn.png\" width=\"1000\" height=\"auto\" />\n",
    "\n",
    "(Source: [Fisseha Berhane, n.d.](https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class LSTMUnit:\n",
    "    \n",
    "    def __init__(self, x_n=3, y_n=2, a_n=5):\n",
    "        # Weight for forget gate\n",
    "        self.f_weights = np.random.randn(a_n, a_n + x_n)\n",
    "        # Weight for update gate\n",
    "        self.i_weights = np.random.randn(a_n, a_n + x_n)\n",
    "        # Weight for candidate \"tanh\"\n",
    "        self.c_weights = np.random.randn(a_n, a_n + x_n)\n",
    "        # Weight for output gate\n",
    "        self.o_weights = np.random.randn(a_n, a_n + x_n)\n",
    "        # Weight for relating the hidden-state to output\n",
    "        self.y_weights = np.random.randn(y_n, a_n)\n",
    "        # Bias for forget gate\n",
    "        self.f_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for update gate\n",
    "        self.i_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for candidate \"tanh\"\n",
    "        self.c_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for output gate\n",
    "        self.o_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for relating the hidden-state to output\n",
    "        self.y_bias = np.random.randn(y_n, 1)\n",
    "\n",
    "    def cell_forward(self, x_values, a_prev, c_prev):\n",
    "        x_n, x_m = x_values.shape\n",
    "        y_n, a_n = self.y_weights.shape\n",
    "        \n",
    "        # Concatenate a_prev with x_values\n",
    "        concat = np.zeros((a_n + x_n, x_m))\n",
    "        concat[:a_n,:] = a_prev\n",
    "        concat[a_n:,:] = x_values\n",
    "        \n",
    "        forget_v = sigmoid(np.dot(self.f_weights, concat) + self.f_bias)\n",
    "        update_v = sigmoid(np.dot(self.i_weights, concat) + self.i_bias)\n",
    "        tanh_v = np.tanh(np.dot(self.c_weights, concat) + self.c_bias)\n",
    "        output_v = sigmoid(np.dot(self.o_weights, concat) + self.o_bias)\n",
    "\n",
    "        c_next = forget_v * c_prev + update_v * tanh_v\n",
    "        a_next = output_v * np.tanh(c_next)\n",
    "\n",
    "        y_pred = softmax(np.dot(self.y_weights, a_next) + self.y_bias)\n",
    "\n",
    "        return dict(a_prev=a_prev, \n",
    "                    a_next=a_next, \n",
    "                    c_prev=c_prev, \n",
    "                    c_next=c_next,\n",
    "                    forget=forget_v,\n",
    "                    update=update_v,\n",
    "                    tanh=tanh_v,\n",
    "                    output=output_v,\n",
    "                    y_pred=y_pred)\n",
    "\n",
    "    def cell_backward(self, x_values, hs, a_next_d, c_next_d):\n",
    "        a_n = hs['a_next'].shape[0]\n",
    "\n",
    "        output_v_d = a_next_d * np.tanh(hs['c_next']) * \\\n",
    "                     hs['output'] * (1 - hs['output'])\n",
    "        \n",
    "        tanh_v_d = (c_next_d * hs['update'] + hs['output'] * (1 - np.square(np.tanh(hs['c_next']))) * \\\n",
    "                     hs['update'] * a_next_d) * \\\n",
    "                   (1 - np.square(hs['tanh']))\n",
    "        \n",
    "        update_v_d = (c_next_d * hs['tanh'] + hs['output'] * (1 - np.square(np.tanh(hs['c_next']))) * \\\n",
    "                       hs['tanh'] * a_next_d) * \\\n",
    "                     hs['update'] * (1 - hs['update'])\n",
    "        \n",
    "        forget_v_d = (c_next_d * hs['c_prev'] + hs['output'] * (1 - np.square(np.tanh(hs['c_next']))) * \\\n",
    "                       hs['c_prev'] * a_next_d) * \\\n",
    "                     hs['forget'] * (1 - hs['forget'])\n",
    "\n",
    "        concat = np.concatenate((hs['a_prev'], x_values), axis=0)\n",
    "\n",
    "        f_weights_d = np.dot(forget_v_d, concat.T)\n",
    "        i_weights_d = np.dot(update_v_d, concat.T)\n",
    "        c_weights_d = np.dot(tanh_v_d, concat.T)\n",
    "        o_weights_d = np.dot(output_v_d, concat.T)\n",
    "\n",
    "        f_bias_d = np.sum(forget_v_d, axis=1 ,keepdims=True)\n",
    "        i_bias_d = np.sum(update_v_d, axis=1, keepdims=True)\n",
    "        c_bias_d = np.sum(tanh_v_d, axis=1,  keepdims=True)\n",
    "        o_bias_d = np.sum(output_v_d, axis=1, keepdims=True)\n",
    "\n",
    "        a_prev_d = np.dot(self.f_weights[:, :a_n].T, forget_v_d) + \\\n",
    "                   np.dot(self.i_weights[:, :a_n].T, update_v_d) + \\\n",
    "                   np.dot(self.c_weights[:, :a_n].T, tanh_v_d) + \\\n",
    "                   np.dot(self.o_weights[:, :a_n].T, output_v_d)\n",
    "\n",
    "        c_prev_d = c_next_d * hs['forget'] + hs['output'] * \\\n",
    "                   (1 - np.square(np.tanh(hs['c_next']))) * hs['forget'] * a_next_d\n",
    "\n",
    "        x_values_d = np.dot(self.f_weights[:, a_n:].T, forget_v_d) + \\\n",
    "                     np.dot(self.i_weights[:, a_n:].T, update_v_d) + \\\n",
    "                     np.dot(self.c_weights[:, a_n:].T, tanh_v_d) + \\\n",
    "                     np.dot(self.o_weights[:, a_n:].T, output_v_d)\n",
    "\n",
    "        return dict(output_d=output_v_d,\n",
    "                    tanh_d=tanh_v_d,\n",
    "                    update_d=update_v_d,\n",
    "                    forget_d=forget_v_d,\n",
    "                    f_weights_d=f_weights_d,\n",
    "                    i_weights_d=i_weights_d,\n",
    "                    c_weights_d=c_weights_d,\n",
    "                    o_weights_d=o_weights_d,\n",
    "                    f_bias_d=f_bias_d,\n",
    "                    i_bias_d=i_bias_d,\n",
    "                    c_bias_d=c_bias_d,\n",
    "                    o_bias_d=o_bias_d,\n",
    "                    a_prev_d=a_prev_d,\n",
    "                    c_prev_d=c_prev_d,\n",
    "                    x_values_d=x_values_d)\n",
    "\n",
    "    def forward_pass(self, x_time_values, a_init=None):\n",
    "        y_size, a_n = self.y_weights.shape\n",
    "        _, x_m, features_n = x_time_values.shape\n",
    "\n",
    "        a = np.zeros((a_n, x_m, features_n))\n",
    "        c = a\n",
    "        y_pred = np.zeros((y_size, x_m, features_n))\n",
    "        hss = []\n",
    "    \n",
    "        a_next = a_init if a_init is not None else np.zeros((a_n, x_m))\n",
    "        c_next = np.zeros(a_next.shape)\n",
    "\n",
    "        for t in range(features_n):\n",
    "            uc = self.cell_forward(x_time_values[:,:,t], a_next, c_next)\n",
    "            a_next = uc['a_next']\n",
    "            c_next = uc['c_next']\n",
    "            a[:,:,t] = uc['a_next']\n",
    "            c[:,:,t] = uc['c_next']\n",
    "            y_pred[:,:,t] = uc['y_pred']\n",
    "            hss.append(uc)\n",
    "\n",
    "        return dict(a=a, c=c, y_pred=y_pred, hss=hss)\n",
    "\n",
    "    def backward_pass(self, x_time_values, hss, a_init_d=None, c_init_d=None):\n",
    "        _, x_m, features_n = x_time_values.shape\n",
    "        a_n = self.y_weights.shape[1]\n",
    "\n",
    "        x_time_values_d = np.zeros(x_time_values.shape)\n",
    "\n",
    "        f_weights_d_cum = np.zeros(self.f_weights.shape)\n",
    "        i_weights_d_cum = np.zeros(self.i_weights.shape)\n",
    "        c_weights_d_cum = np.zeros(self.c_weights.shape)\n",
    "        o_weights_d_cum = np.zeros(self.o_weights.shape)\n",
    "\n",
    "        f_bias_d_cum = np.zeros(self.f_bias.shape)\n",
    "        i_bias_d_cum = np.zeros(self.i_bias.shape)\n",
    "        c_bias_d_cum = np.zeros(self.c_bias.shape)\n",
    "        o_bias_d_cum = np.zeros(self.o_bias.shape)\n",
    "\n",
    "        a_d = a_init_d if a_init_d is not None else np.zeros((a_n, x_m, features_n))\n",
    "        c_d = c_init_d if c_init_d is not None else np.zeros(a_d.shape)\n",
    "\n",
    "        a_prev_d = np.zeros((a_n, x_m))\n",
    "        c_prev_d = np.zeros(a_prev_d.shape)\n",
    "\n",
    "        for t in reversed(range(features_n)):\n",
    "            cell_gradient = self.cell_backward(x_time_values[:,:,t], hss[t], a_prev_d + a_d[:,:,t], c_prev_d + c_d[:,:,t])\n",
    "            \n",
    "            x_time_values_d[:,:,t] = cell_gradient['x_values_d']\n",
    "            f_weights_d_cum += cell_gradient['f_weights_d']\n",
    "            i_weights_d_cum += cell_gradient['i_weights_d']\n",
    "            c_weights_d_cum += cell_gradient['c_weights_d']\n",
    "            o_weights_d_cum += cell_gradient['o_weights_d']\n",
    "            \n",
    "            \n",
    "            f_bias_d_cum += cell_gradient['f_bias_d']\n",
    "            i_bias_d_cum += cell_gradient['i_bias_d']\n",
    "            c_bias_d_cum += cell_gradient['c_bias_d']\n",
    "            o_bias_d_cum += cell_gradient['o_bias_d']\n",
    "\n",
    "            a_prev_d = cell_gradient['a_prev_d']\n",
    "            c_prev_d = cell_gradient['c_prev_d']\n",
    "        \n",
    "        return dict(f_weights_d=f_weights_d_cum,\n",
    "                    i_weights_d=i_weights_d_cum,\n",
    "                    c_weights_d=c_weights_d_cum,\n",
    "                    o_weights_d=o_weights_d_cum,\n",
    "                    f_bias_d=f_bias_d_cum,\n",
    "                    i_bias_d=i_bias_d_cum,\n",
    "                    c_bias_d=c_bias_d_cum,\n",
    "                    o_bias_d=o_bias_d_cum,\n",
    "                    a_prev_d=a_prev_d,\n",
    "                    c_prev_d=c_prev_d,\n",
    "                    x_values_d=x_time_values_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Preview pass-through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward pass (peak @ [0][0]) ===\n",
      "Value at time 1:\n",
      "    a(t-1) = 0.0, \n",
      "    a(t) = -0.08343433206127393, \n",
      "    c(t-1) = 0.0, \n",
      "    c(t) = -0.08343433206127393,\n",
      "    y_pred = 0.734875861137326,\n",
      "\n",
      "    forget gate=0.8075218579825069,\n",
      "    update gate=0.3062247180809734,\n",
      "    tanh=-0.27246112784145604,\n",
      "    output gate=0.9369490290591823\n",
      "\n",
      "Value at time 2:\n",
      "    a(t-1) = -0.07799282351874157, \n",
      "    a(t) = 0.29223704943478285, \n",
      "    c(t-1) = -0.08343433206127393, \n",
      "    c(t) = 0.29223704943478285,\n",
      "    y_pred = 0.6521032449346058,\n",
      "\n",
      "    forget gate=0.18974605150000293,\n",
      "    update gate=0.3081022533091858,\n",
      "    tanh=0.9998900728382465,\n",
      "    output gate=0.7921089450360703\n",
      "\n",
      "Value at time 3:\n",
      "    a(t-1) = 0.22511141400895762, \n",
      "    a(t) = 0.35817201453850156, \n",
      "    c(t-1) = 0.29223704943478285, \n",
      "    c(t) = 0.35817201453850156,\n",
      "    y_pred = 0.6193589787769862,\n",
      "\n",
      "    forget gate=0.10914937710662423,\n",
      "    update gate=0.3266801882861681,\n",
      "    tanh=0.9987582177447643,\n",
      "    output gate=0.6912641796134577\n",
      "\n",
      "Value at time 4:\n",
      "    a(t-1) = 0.2375203627986793, \n",
      "    a(t) = 0.5094301773974056, \n",
      "    c(t-1) = 0.35817201453850156, \n",
      "    c(t) = 0.5094301773974056,\n",
      "    y_pred = 0.3885839580058404,\n",
      "\n",
      "    forget gate=0.6610191649041838,\n",
      "    update gate=0.852198143121948,\n",
      "    tanh=0.31996269137152683,\n",
      "    output gate=0.8525601072942414\n",
      "\n",
      "Values for weights and biases:\n",
      "    W_f = -0.024616955875778355\n",
      "    W_o = -0.4982135636310781\n",
      "    W_i = -0.5170944579202279\n",
      "    W_c = -0.40087819178892664\n",
      "    W_y = -0.7819116826868007\n",
      "    b_f = -0.18657899351146628\n",
      "    b_o = 1.622849085954001\n",
      "    b_i = -0.2973618827735036\n",
      "    b_c = 2.0657833202188343\n",
      "    b_y = 1.1234121620219353\n",
      "\n",
      "=== Backward pass (peak @ [0][0]) ===\n",
      "Derivative at time 4:\n",
      "    x(t) = 0.7167239817009771\n",
      "\n",
      "Derivative at time 3:\n",
      "    x(t) = 0.04211011152469907\n",
      "\n",
      "Derivative at time 2:\n",
      "    x(t) = -0.07129706062310935\n",
      "\n",
      "Derivative at time 1:\n",
      "    x(t) = -0.2460779866281615\n",
      "\n",
      "Derivatives for weights and biases:\n",
      "    W_f = 0.16946502919976614,\n",
      "    W_c = 0.13708071907549005,\n",
      "    W_i = 0.2830203770370697,\n",
      "    W_o = 0.1766020071679656,\n",
      "    b_f = 0.11883797424669484,\n",
      "    b_c = -1.5558876547259022,\n",
      "    b_i = 0.2584448169778234,\n",
      "    b_o = 0.2584448169778234\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "no_of_features = 4\n",
    "x_train = np.random.randn(3, 10, no_of_features)\n",
    "\n",
    "unit = LSTMUnit(x_n=x_train.shape[0], y_n=2, a_n=5)\n",
    "\n",
    "print('=== Forward pass (peak @ [0][0]) ===')\n",
    "\n",
    "fp_result = unit.forward_pass(x_train)\n",
    "\n",
    "for t in range(x_train.shape[2]):\n",
    "    print(f\"\"\"Value at time {t+1}:\n",
    "    a(t-1) = {fp_result['hss'][t]['a_prev'][0][0]}, \n",
    "    a(t) = {fp_result['a'][:,:,t][0][0]}, \n",
    "    c(t-1) = {fp_result['hss'][t]['c_prev'][0][0]}, \n",
    "    c(t) = {fp_result['c'][:,:,t][0][0]},\n",
    "    y_pred = {fp_result['y_pred'][:,:,t][0][0]},\n",
    "\n",
    "    forget gate={fp_result['hss'][t]['forget'][0][0]},\n",
    "    update gate={fp_result['hss'][t]['update'][0][0]},\n",
    "    tanh={fp_result['hss'][t]['tanh'][0][0]},\n",
    "    output gate={fp_result['hss'][t]['output'][0][0]}\n",
    "\"\"\")\n",
    "    \n",
    "print(f\"\"\"Values for weights and biases:\n",
    "    W_f = {unit.f_weights[0][0]}\n",
    "    W_o = {unit.o_weights[0][0]}\n",
    "    W_i = {unit.i_weights[0][0]}\n",
    "    W_c = {unit.c_weights[0][0]}\n",
    "    W_y = {unit.y_weights[0][0]}\n",
    "    b_f = {unit.f_bias[0][0]}\n",
    "    b_o = {unit.o_bias[0][0]}\n",
    "    b_i = {unit.i_bias[0][0]}\n",
    "    b_c = {unit.c_bias[0][0]}\n",
    "    b_y = {unit.y_bias[0][0]}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print('=== Backward pass (peak @ [0][0]) ===')\n",
    "\n",
    "bp = unit.backward_pass(x_train, fp_result['hss'], a_init_d=np.random.randn(*fp_result['a'].shape), \n",
    "                                                   c_init_d=np.random.randn(*fp_result['c'].shape))\n",
    "\n",
    "for t in reversed(range(x_train.shape[2])):\n",
    "    print(f\"\"\"Derivative at time {t+1}:\n",
    "    x(t) = {bp['x_values_d'][:,:,t][0][0]}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"Derivatives for weights and biases:\n",
    "    W_f = {bp['f_weights_d'][0][0]},\n",
    "    W_c = {bp['c_weights_d'][0][0]},\n",
    "    W_i = {bp['i_weights_d'][0][0]},\n",
    "    W_o = {bp['o_weights_d'][0][0]},\n",
    "    b_f = {bp['f_bias_d'][0][0]},\n",
    "    b_c = {bp['c_bias_d'][0][0]},\n",
    "    b_i = {bp['o_bias_d'][0][0]},\n",
    "    b_o = {bp['o_bias_d'][0][0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Create the GRU Unit (Forward only)\n",
    "\n",
    "Here's the basic idea of the cell.\n",
    "\n",
    "<img src=\"http://media5.datahacker.rs/2020/09/69-2048x1292.jpg\" width=\"400\" height=\"auto\" />\n",
    "\n",
    "(Source: [Strahinja Zivkovic, 2020](http://datahacker.rs/005-rnn-tackling-vanishing-gradients-with-gru-and-lstm/) - _Note that diagram omits the reset gate. Refer to [this](https://d2l.ai/chapter_recurrent-modern/gru.html) for better overview_)\n",
    "\n",
    "Credits: [d2l.ai, n.d.](https://d2l.ai/chapter_recurrent-modern/gru.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUUnit:\n",
    "    \n",
    "    def __init__(self, x_n=3, x_m=10, y_n=2, a_n=5):\n",
    "        # Weight for reset gate\n",
    "        self.r_weights = np.random.randn(a_n, a_n)\n",
    "        # Weight for update gate\n",
    "        self.i_weights = np.random.randn(a_n, x_n)\n",
    "        # Weight for candidate \"tanh\"\n",
    "        self.c_weights = np.random.randn(a_n, x_n)\n",
    "        self.ch_weights = np.random.randn(a_n, 1)\n",
    "        # Weight for relating the hidden-state to output\n",
    "        self.y_weights = np.random.randn(y_n, a_n)\n",
    "        # Bias for reset gate\n",
    "        self.r_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for update gate\n",
    "        self.i_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for candidate \"tanh\"\n",
    "        self.c_bias = np.random.randn(a_n, 1)\n",
    "        # Bias for relating the hidden-state to output\n",
    "        self.y_bias = np.random.randn(y_n, 1)\n",
    "\n",
    "    def cell_forward(self, x_values, a_prev):\n",
    "        #a_next = np.tanh(np.dot(self.aa_weights, a_prev) + np.dot(self.ax_weights, x_values) + self.a_bias)\n",
    "        #y_pred = softmax(np.dot(self.ya_weights, a_next) + self.y_bias)\n",
    "\n",
    "        #return dict(a_prev=a_prev, a_next=a_next, y_pred=y_pred)\n",
    "\n",
    "        x_n, x_m = x_values.shape\n",
    "        y_n, a_n = self.y_weights.shape\n",
    "\n",
    "        reset_value = sigmoid(np.dot(self.r_weights, a_prev) + self.r_bias)\n",
    "        update_value = sigmoid(np.dot(self.i_weights, x_values) + self.i_bias)\n",
    "        tanh_value = np.tanh(np.dot(self.c_weights, x_values) + (reset_value * a_prev * self.ch_weights) + self.c_bias)\n",
    "        \n",
    "        a_next = update_value * a_prev + (1 - update_value) * tanh_value\n",
    "        y_pred = softmax(np.dot(self.y_weights, a_prev) + self.y_bias)\n",
    "        \n",
    "        return dict(a_prev=a_prev,\n",
    "                    a_next=a_next,\n",
    "                    y_pred=y_pred,\n",
    "                    reset=reset_value,\n",
    "                    update=update_value,\n",
    "                    tanh=tanh_value)\n",
    "\n",
    "    def forward_pass(self, x_time_values, a_init=None):\n",
    "        y_size, a_n = self.y_weights.shape\n",
    "        _, x_m, features_n = x_time_values.shape\n",
    "\n",
    "        a = np.zeros((a_n, x_m, features_n))\n",
    "        y_pred = np.zeros((y_size, x_m, features_n))\n",
    "        hss = []\n",
    "    \n",
    "        a_next = a_init if a_init is not None else np.zeros((a_n, x_m))\n",
    "\n",
    "        for t in range(features_n):\n",
    "            uc = self.cell_forward(x_time_values[:,:,t], a_next)\n",
    "            a_next = uc['a_next']\n",
    "            a[:,:,t] = uc['a_next']\n",
    "            y_pred[:,:,t] = uc['y_pred']\n",
    "            hss.append(uc)\n",
    "\n",
    "        return dict(a=a, y_pred=y_pred, hss=hss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Preview pass-through (Forward only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward pass (peak @ [0][0]) ===\n",
      "Value at time 1:\n",
      "    a(t-1) = 0.0, \n",
      "    a(t) = -0.6624936673915963, \n",
      "    y_pred = 0.47253584733537496,\n",
      "\n",
      "    reset gate=0.7613942293321033,\n",
      "    update gate=0.17419602049370927,\n",
      "    tanh=-0.8022408269183566\n",
      "\n",
      "Value at time 2:\n",
      "    a(t-1) = -0.6624936673915963, \n",
      "    a(t) = 0.5197961377421829, \n",
      "    y_pred = 0.5676720695982849,\n",
      "\n",
      "    reset gate=0.7239697979279207,\n",
      "    update gate=0.12955500229112404,\n",
      "    tanh=0.6957653934861379\n",
      "\n",
      "Value at time 3:\n",
      "    a(t-1) = 0.5197961377421829, \n",
      "    a(t) = 0.5165426982099552, \n",
      "    y_pred = 0.4256564095305796,\n",
      "\n",
      "    reset gate=0.7891613891909318,\n",
      "    update gate=0.16206445587942556,\n",
      "    tanh=0.5159134530237035\n",
      "\n",
      "Value at time 4:\n",
      "    a(t-1) = 0.5165426982099552, \n",
      "    a(t) = -0.6265279462002945, \n",
      "    y_pred = 0.47346074188071147,\n",
      "\n",
      "    reset gate=0.8851338171573095,\n",
      "    update gate=0.1994219899747511,\n",
      "    tanh=-0.9112639990714243\n",
      "\n",
      "Values for weights and biases:\n",
      "    W_r = -0.024616955875778355\n",
      "    W_i = 0.13770120999738608\n",
      "    W_c = -0.5170944579202279\n",
      "    W_ch = -0.22631424251360518\n",
      "    W_y = -1.2725587552459943\n",
      "    b_r = 1.1603385699937696\n",
      "    b_i = -1.6274383406162574\n",
      "    b_c = -0.40087819178892664\n",
      "    b_y = -1.7606885603987834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "no_of_features = 4\n",
    "x_train = np.random.randn(3, 10, no_of_features)\n",
    "\n",
    "unit = GRUUnit(x_n=x_train.shape[0], x_m=10, y_n=2, a_n=5)\n",
    "\n",
    "print('=== Forward pass (peak @ [0][0]) ===')\n",
    "\n",
    "fp_result = unit.forward_pass(x_train)\n",
    "\n",
    "for t in range(x_train.shape[2]):\n",
    "    print(f\"\"\"Value at time {t+1}:\n",
    "    a(t-1) = {fp_result['hss'][t]['a_prev'][0][0]}, \n",
    "    a(t) = {fp_result['a'][:,:,t][0][0]}, \n",
    "    y_pred = {fp_result['y_pred'][:,:,t][0][0]},\n",
    "\n",
    "    reset gate={fp_result['hss'][t]['reset'][0][0]},\n",
    "    update gate={fp_result['hss'][t]['update'][0][0]},\n",
    "    tanh={fp_result['hss'][t]['tanh'][0][0]}\n",
    "\"\"\")\n",
    "    \n",
    "print(f\"\"\"Values for weights and biases:\n",
    "    W_r = {unit.r_weights[0][0]}\n",
    "    W_i = {unit.i_weights[0][0]}\n",
    "    W_c = {unit.c_weights[0][0]}\n",
    "    W_ch = {unit.ch_weights[0][0]}\n",
    "    W_y = {unit.y_weights[0][0]}\n",
    "    b_r = {unit.r_bias[0][0]}\n",
    "    b_i = {unit.i_bias[0][0]}\n",
    "    b_c = {unit.c_bias[0][0]}\n",
    "    b_y = {unit.y_bias[0][0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON0YxX/oky4tPbqCLnFjWD",
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
