{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "## 1. Import libraries and preprocess dataset\n",
    "\n",
    "To visualize (or recall) what shear does,\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Academ_Study_about_a_periodic_tiling_by_regular_polygons.svg/330px-Academ_Study_about_a_periodic_tiling_by_regular_polygons.svg.png\" width=\"200\" height=\"auto\" />\n",
    "\n",
    "(Source: [Baelde, 2013](https://en.wikipedia.org/wiki/Shear_mapping))\n",
    "\n",
    "What rescale does,\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Idrissa-Coulibaly/publication/263179118/figure/fig2/AS:669296715890712@1536584180478/Multi-scale-example-processing-by-octave.png\" width=\"300\" height=\"auto\" />\n",
    "\n",
    "(Source: [Idrissa Coulibaly et., 2014](https://www.researchgate.net/publication/263179118_A_novel_approach_for_road_damage_assessment_in_case_of_major_disaster_based_on_multi-resolution_analysis))\n",
    "\n",
    "What zoom range does,\n",
    "\n",
    "<img src=\"https://s3.ap-south-1.amazonaws.com/s3.studytonight.com/curious/uploads/pictures/1611473961-74364.png\" width=\"450\" height=\"auto\" />\n",
    "\n",
    "(Source: [@shekharpandey, 2021](https://www.studytonight.com/post/random-zoom-image-augmentation-keras-imagedatagenerator))\n",
    "\n",
    "What flipping does,\n",
    "\n",
    "<img src=\"https://desktop.arcgis.com/en/arcmap/latest/tools/data-management-toolbox/GUID-5EA301F4-0E6D-47DE-9F28-D9E754BD8784-web.gif\" width=\"300\" height=\"auto\" />\n",
    "\n",
    "(Source: [arcgis.com, n.d.](https://desktop.arcgis.com/en/arcmap/latest/tools/data-management-toolbox/changing-the-orientation-of-a-raster.htm))\n",
    "\n",
    "### Why do we use the image data generator?\n",
    "\n",
    "This adds synthetic data points, which exposes the model to additional variations without the cost of collecting and annotating more data. It reduces overfitting and improves the model's ability to generalize.\n",
    "\n",
    "Intuitively, flipping an image object should be equally recognizable as its mirror image. Zooming reduces the contribution of the background in the CNN's decision for locating where an object is. Shearing should also allow the CNN to recognize it despite minor distortions.\n",
    "\n",
    "Rescaling (or normalizing) (done by setting the `target_size`) to a fixed image size also reduces training time and makes the detection much easier.\n",
    "\n",
    "The `batch_size` is the chunk size of the data for each epoch (due to huge volumes of data), and the iterations is the number of runs in each epoch. For `flow_from_directory`, the classes are determined from the respective folders name. In our case, we only need to do a binary classification since there are only 2 classes (in which a reference could be thought of sigmoid not softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Parameters\n",
    "input_n_size = (64, 64)\n",
    "batch_size = 32\n",
    "\n",
    "# Training dataset\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,         \n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size=input_n_size,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 class_mode='binary')\n",
    "\n",
    "# Test dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size=input_n_size,\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "## 2. Create the CNN model\n",
    "\n",
    "Similar reference with AlexNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# L1: Convolution Layer\n",
    "L1_f_size = 3 # 3 Ã— 3 filter\n",
    "L1_params = {\n",
    "    'filters': 32, \n",
    "    'kernel_size': L1_f_size, \n",
    "    'activation': 'relu', \n",
    "    'input_shape': list(input_n_size) + [L1_f_size]\n",
    "}\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(**L1_params))\n",
    "\n",
    "# L2: Max-Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# L3: Convolution Layer\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "\n",
    "# L4: Max-Pooling Layer\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# L5-pre: Flattening layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# L5: Full connection layer\n",
    "model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# L6: Output layer\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile with the descent algorithm and loss function\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "## 3. Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUj1W4PJptta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 80s 311ms/step - loss: 0.6702 - accuracy: 0.5863 - val_loss: 0.6169 - val_accuracy: 0.6630\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 62s 247ms/step - loss: 0.5980 - accuracy: 0.6816 - val_loss: 0.5550 - val_accuracy: 0.7260\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 54s 216ms/step - loss: 0.5669 - accuracy: 0.7064 - val_loss: 0.5340 - val_accuracy: 0.7300\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 56s 224ms/step - loss: 0.5357 - accuracy: 0.7287 - val_loss: 0.5308 - val_accuracy: 0.7405\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 51s 205ms/step - loss: 0.5136 - accuracy: 0.7462 - val_loss: 0.5224 - val_accuracy: 0.7465\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 54s 214ms/step - loss: 0.5001 - accuracy: 0.7552 - val_loss: 0.4823 - val_accuracy: 0.7630\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 53s 210ms/step - loss: 0.4766 - accuracy: 0.7673 - val_loss: 0.4806 - val_accuracy: 0.7680\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.4609 - accuracy: 0.7807 - val_loss: 0.4801 - val_accuracy: 0.7735\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.4551 - accuracy: 0.7857 - val_loss: 0.4720 - val_accuracy: 0.7870\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 53s 212ms/step - loss: 0.4448 - accuracy: 0.7814 - val_loss: 0.5711 - val_accuracy: 0.7265\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 52s 206ms/step - loss: 0.4267 - accuracy: 0.8006 - val_loss: 0.4579 - val_accuracy: 0.7920\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.4151 - accuracy: 0.8073 - val_loss: 0.4803 - val_accuracy: 0.7840\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.4028 - accuracy: 0.8134 - val_loss: 0.4720 - val_accuracy: 0.7865\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 52s 210ms/step - loss: 0.4014 - accuracy: 0.8160 - val_loss: 0.4665 - val_accuracy: 0.7935\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.3940 - accuracy: 0.8176 - val_loss: 0.4551 - val_accuracy: 0.7965\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 52s 209ms/step - loss: 0.3896 - accuracy: 0.8188 - val_loss: 0.4671 - val_accuracy: 0.7970\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 52s 208ms/step - loss: 0.3730 - accuracy: 0.8314 - val_loss: 0.4867 - val_accuracy: 0.7860\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 52s 207ms/step - loss: 0.3640 - accuracy: 0.8356 - val_loss: 0.4561 - val_accuracy: 0.7925\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3475 - accuracy: 0.8457 - val_loss: 0.5746 - val_accuracy: 0.7480\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 52s 206ms/step - loss: 0.3560 - accuracy: 0.8418 - val_loss: 0.4769 - val_accuracy: 0.7970\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 53s 210ms/step - loss: 0.3401 - accuracy: 0.8487 - val_loss: 0.4861 - val_accuracy: 0.7920\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 51s 203ms/step - loss: 0.3299 - accuracy: 0.8526 - val_loss: 0.4806 - val_accuracy: 0.7955\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 53s 210ms/step - loss: 0.3231 - accuracy: 0.8540 - val_loss: 0.4624 - val_accuracy: 0.8080\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 51s 206ms/step - loss: 0.3138 - accuracy: 0.8641 - val_loss: 0.4822 - val_accuracy: 0.7980\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 53s 211ms/step - loss: 0.3159 - accuracy: 0.8640 - val_loss: 0.5507 - val_accuracy: 0.7875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c923ad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=training_set, validation_data=test_set, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## 4. Predict the following observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsSiWEJY1BPB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  {'cats': 0, 'dogs': 1} \n",
      "\n",
      "Prediction: dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size=input_n_size)\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "# Make prediction\n",
    "result = model.predict(test_image)\n",
    "\n",
    "print('Classes: ', training_set.class_indices, '\\n')\n",
    "print(f'Prediction: {\"dog\" if result[0][0] == 1 else \"cat\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyON0YxX/oky4tPbqCLnFjWD",
   "collapsed_sections": [],
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
