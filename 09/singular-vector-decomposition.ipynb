{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dataset = pd.read_csv('wine.csv')\n",
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Standardize dataset\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Getting a sense of SVD\n",
    "\n",
    "To calculate the SVD, imagine having matrix `A`,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*4DncmDEnF9SIYTTrDR0Adw.png\" width=\"650\" height=\"auto\" />\n",
    "\n",
    "Calculate the following eigenvalues and vectors (normalized) from `A^T ⋅ A` and `A ⋅ A^T`,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*RG2fiVNyPDr77eS4qsAjLg.jpeg\" width=\"750\" height=\"auto\" />\n",
    "\n",
    "These will be used to form the matrices for `U`, `S` and `V^T`.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xnlAa8E-c63HnMcTRcb5HA.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))\n",
    "\n",
    "Credits: [Jason Brownlee](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix (m × n) = (142, 13)\n",
      "U matrix (m × n) = (142, 142)\n",
      "S matrix (m × n) = (13,)\n",
      "V^T matrix (m × n) = (13, 13)\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "# A = U * S * V^T\n",
    "A = x_train\n",
    "U, S, VT = svd(A)\n",
    "\n",
    "print(f'A matrix (m × n) = {A.shape}')\n",
    "print(f'U matrix (m × n) = {U.shape}')\n",
    "print(f'S matrix (m × n) = {S.shape}')\n",
    "print(f'V^T matrix (m × n) = {VT.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Reformulating SVD\n",
    "\n",
    "Reformulating it, we can represent it as such,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*0LEG-KOZYYYsaXnQxMCkXA.gif\" width=\"500\" height=\"auto\" />\n",
    "\n",
    "And each vectors can be formulated as,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*KGcqnL20ihPN4RDyLhQzXA.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "Giving this linear combination,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*0n2-o06c_j42d0MJo7igYQ.gif\" width=\"500\" height=\"auto\" />\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstructed matrix B,\n",
      "\n",
      "A(1) = [ 0.87668336  0.79842885  0.64412971  0.12974277  0.48853231 -0.70326216\n",
      " -1.42846826  1.0724566  -1.36820277  0.35193216  0.0290166  -1.06412236\n",
      " -0.2059076 ]\n",
      "\n",
      "B(1) = [ 0.87668336  0.79842885  0.64412971  0.12974277  0.48853231 -0.70326216\n",
      " -1.42846826  1.0724566  -1.36820277  0.35193216  0.0290166  -1.06412236\n",
      " -0.2059076 ]\n",
      "\n",
      "A(n) = [ 1.4610222   0.12361993  0.42085937  0.12974277 -0.63831583 -0.94935192\n",
      " -1.28450624  0.60097413 -0.62134527  1.96237659 -1.45501034 -1.2168803\n",
      " -0.2719767 ]\n",
      "\n",
      "B(n) = [ 1.4610222   0.12361993  0.42085937  0.12974277 -0.63831583 -0.94935192\n",
      " -1.28450624  0.60097413 -0.62134527  1.96237659 -1.45501034 -1.2168803\n",
      " -0.2719767 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct the matrix and gives back the same, since the last element is 0\n",
    "# This calculates the same way as 2a.\n",
    "# Convert U(m × m) . sigma(n × n) . V^T(n × n)\n",
    "# Into    U(m × m) . sigma(m × n) . V^T(n × n)\n",
    "# Create the m × n matrix\n",
    "sigma = np.zeros(A.shape)\n",
    "# Populate the n × n matrix\n",
    "sigma[:A.shape[1], :A.shape[1]] = np.diag(S)\n",
    "# Reconstruct matrix\n",
    "B = U.dot(sigma.dot(VT))\n",
    "print('\\nReconstructed matrix B,\\n')\n",
    "print(f'A(1) = {A[0]}\\n')\n",
    "print(f'B(1) = {B[0]}\\n')\n",
    "print(f'A(n) = {A[-1]}\\n')\n",
    "print(f'B(n) = {B[-1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Pseudo-Inverse SVD\n",
    "\n",
    "For a linear equation system, compute the inverse of a square matrix `A` to solve `x` where,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*a1inq-_XL9WHTCsxzHpamQ.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "But as not all matrices are invertible, it will be unlikely to find an exact solution. Hence to find the best-fit solution, compute the following pseudoinverse `A+` which minimizes the least square error,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*aQ2MqySUZnIbCIrxfH1G8Q.png\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "Which solution for `x` can be estimated as,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*lF1z-LodZHA3834kseswYw.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "Leading to,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ClzObIIjZyQDb9svX4FjjQ.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*mu9Z_NVYd_3CXwhbERVB8Q.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "For illustration purposes (see how the `D+` is constructed for none invertible matrices),\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xxatolWVNPjMCUEEWLfvyg.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix (m × n) = (142, 13)\n",
      "A+ matrix (m × n) = (13, 142)\n",
      "\n",
      "Reconstructed matrix A+,\n",
      "\n",
      "A(1) = [ 0.87668336  0.79842885  0.64412971  0.12974277  0.48853231 -0.70326216\n",
      " -1.42846826  1.0724566  -1.36820277  0.35193216  0.0290166  -1.06412236\n",
      " -0.2059076 ]\n",
      "\n",
      "A+(1) = [ 0.01306503  0.00318994  0.00712028 -0.00720144  0.00405339  0.01426846\n",
      " -0.03178858  0.00139091 -0.0028464  -0.00606619  0.01362248 -0.00143495\n",
      " -0.00448885]\n",
      "\n",
      "A(n) = [ 1.4610222   0.12361993  0.42085937  0.12974277 -0.63831583 -0.94935192\n",
      " -1.28450624  0.60097413 -0.62134527  1.96237659 -1.45501034 -1.2168803\n",
      " -0.2719767 ]\n",
      "\n",
      "A+(n) = [ 0.01632071 -0.01043116  0.00713514 -0.00448943 -0.00863543 -0.0025722\n",
      " -0.02188495 -0.00690779  0.0079907   0.00881534 -0.00165585  0.00523781\n",
      " -0.01022904]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B = A+\n",
    "A_PI = np.linalg.pinv(A)\n",
    "\n",
    "print(f'A matrix (m × n) = {A.shape}')\n",
    "print(f'A+ matrix (m × n) = {A_PI.shape}')\n",
    "print('\\nReconstructed matrix A+,\\n')\n",
    "print(f'A(1) = {A[0]}\\n')\n",
    "print(f'A+(1) = {A_PI[:,0].flatten()}\\n')\n",
    "print(f'A(n) = {A[-1]}\\n')\n",
    "print(f'A+(n) = {A_PI[:,-1].flatten()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix (m × n) = (142, 13)\n",
      "A+ matrix (m × n) = (13, 142)\n"
     ]
    }
   ],
   "source": [
    "# This calculates the same way as 2c.\n",
    "# Populate D with n × n diagonal matrix based on A's m × n\n",
    "D = zeros(A.shape)\n",
    "D[:A.shape[1], :A.shape[1]] = diag(1.0 / S)\n",
    "# Calculate pseudoinverse\n",
    "A_PI = VT.T.dot(D.T).dot(U.T)\n",
    "\n",
    "print(f'A matrix (m × n) = {A.shape}')\n",
    "print(f'A+ matrix (m × n) = {A_PI.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T matrix (m × n) = (142, 2)\n",
      "\n",
      "T(1) = [-2.17884511 -1.07218467]\n",
      "\n",
      "T(n) = [-2.44830439 -2.11360296]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(A)\n",
    "\n",
    "T = svd.transform(A)\n",
    "print(f'T matrix (m × n) = {T.shape}')\n",
    "print(f'\\nT(1) = {T[0]}\\n')\n",
    "print(f'T(n) = {T[-1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V^T matrix (m × n) = (13, 13)\n",
      "V^Tk matrix (m × n) = (2, 13)\n",
      "S matrix (m × n) = (142, 13)\n",
      "Sk matrix (m × n) = (142, 2)\n",
      "\n",
      "A matrix (m × n) = (142, 13)\n",
      "B matrix (m × n) = (142, 13)\n",
      "\n",
      "Reconstructed matrix B,\n",
      "\n",
      "A(1) = [ 0.87668336  0.79842885  0.64412971  0.12974277  0.48853231 -0.70326216\n",
      " -1.42846826  1.0724566  -1.36820277  0.35193216  0.0290166  -1.06412236\n",
      " -0.2059076 ]\n",
      "\n",
      "Bk(1) = [ 0.25164834  0.78144298  0.35990517  0.4991524   0.00146148 -0.74057914\n",
      " -0.9091325   0.70745757 -0.59821518  0.80945717 -0.96035831 -0.9923469\n",
      " -0.22063686]\n",
      "\n",
      "A(n) = [ 1.4610222   0.12361993  0.42085937  0.12974277 -0.63831583 -0.94935192\n",
      " -1.28450624  0.60097413 -0.62134527  1.96237659 -1.45501034 -1.2168803\n",
      " -0.2719767 ]\n",
      "\n",
      "Bk(n) = [ 0.73542901  1.08864448  0.69066494  0.5397817   0.23649916 -0.74066258\n",
      " -1.00249893  0.83121646 -0.61088752  1.39202983 -1.32597784 -1.26112931\n",
      "  0.08422657]\n",
      "\n",
      "T = U . Sk (m × n):  (142, 2)\n",
      "T(1) = [2.17884511 1.07218467]\n",
      "T(n) = [2.44830439 2.11360296]\n",
      "\n",
      "T = A . V^Tk (m × n):  (142, 2)\n",
      "T(1) = [2.17884511 1.07218467]\n",
      "T(n) = [2.44830439 2.11360296]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# T = V^k . A\n",
    "# T = U . Sigma^k\n",
    "components_n = 2\n",
    "\n",
    "sigma_k = sigma[:, :components_n]\n",
    "VT_k = VT[:components_n, :]\n",
    "\n",
    "print(f'V^T matrix (m × n) = {VT.shape}')\n",
    "print(f'V^Tk matrix (m × n) = {VT_k.shape}')\n",
    "print(f'S matrix (m × n) = {sigma.shape}')\n",
    "print(f'Sk matrix (m × n) = {sigma_k.shape}')\n",
    "\n",
    "# Reconstruct\n",
    "B_k = U.dot(sigma_k.dot(VT_k))\n",
    "\n",
    "print(f'\\nA matrix (m × n) = {A.shape}')\n",
    "print(f'B matrix (m × n) = {B_k.shape}')\n",
    "print('\\nReconstructed matrix B,\\n')\n",
    "print(f'A(1) = {A[0]}\\n')\n",
    "print(f'Bk(1) = {B_k[0]}\\n')\n",
    "print(f'A(n) = {A[-1]}\\n')\n",
    "print(f'Bk(n) = {B_k[-1]}\\n')\n",
    "\n",
    "T_k = U.dot(sigma_k)\n",
    "print('T = U . Sk (m × n): ', T_k.shape)\n",
    "print(f'T(1) = {T_k[0]}')\n",
    "print(f'T(n) = {T_k[-1]}\\n')\n",
    "\n",
    "T_k = A.dot(VT_k.T)\n",
    "print('T = A . V^Tk (m × n): ', T_k.shape)\n",
    "print(f'T(1) = {T_k[0]}')\n",
    "print(f'T(n) = {T_k[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sample Example\n",
    "\n",
    "For example, we have a matrix contains the return of stock yields traded by different investors.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*FJXUrl22HERjCUe2dR42mA.gif\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "As a fund manager, you want to identify the combination of stocks and investors that have the largest yields.\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/8JGdq.png\" width=\"200\" height=\"auto\" />\n",
    "\n",
    "(Source: [@jeffery_the_wind, 2013](https://math.stackexchange.com/questions/479918/trying-to-check-cov-matrix-calculation-from-svd-using-numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy covariance\n",
      "    cov shape: (5, 5)\n",
      "    cov[1]: [ 0.10379853  0.02392059 -0.0189761   0.00497978 -0.00631434]\n",
      "    cov[n]: [-0.00631434  0.00696324  0.01202206  0.01818566  0.07878676]\n",
      "\n",
      "Using SVD covariance\n",
      "    cov shape: (5, 5)\n",
      "    cov[1]: [ 0.10379853  0.02392059 -0.0189761   0.00497978 -0.00631434]\n",
      "    cov[n]: [-0.00631434  0.00696324  0.01202206  0.01818566  0.07878676]\n",
      "\n",
      "Displaying stocks and their std. covariance\n",
      " [1.42911619 1.39863859 1.13716518 0.98471829 0.69214407]\n",
      "\n",
      "Displaying stocks covariance matrix\n",
      " [[ 0.10379853  0.02392059 -0.0189761   0.00497978 -0.00631434]\n",
      " [ 0.02392059  0.07812426 -0.02002206 -0.00925129  0.00696324]\n",
      " [-0.0189761  -0.02002206  0.04652574 -0.00608548  0.01202206]\n",
      " [ 0.00497978 -0.00925129 -0.00608548  0.11404228  0.01818566]\n",
      " [-0.00631434  0.00696324  0.01202206  0.01818566  0.07878676]]\n",
      "\n",
      "Selecting best stocks with higher combination yields\n",
      " [1.42911619 1.39863859] ['GOOG', 'AMZN']\n",
      "\n",
      "Gives us related covariance vectors (relationship amongst variables)\n",
      " [[ 0.10379853  0.02392059]\n",
      " [ 0.02392059  0.07812426]\n",
      " [-0.0189761  -0.02002206]\n",
      " [ 0.00497978 -0.00925129]\n",
      " [-0.00631434  0.00696324]]\n"
     ]
    }
   ],
   "source": [
    "stocks = ['GOOG', 'AMZN', 'FB', 'SNAP', 'APPL', 'TESLA', 'MSFT']\n",
    "stock_yields = np.array([\n",
    "    [0.5,  0.2,  0.1, 0.04, 0.3],\n",
    "    [0,    0,    0.2, 0.8,  0.25],\n",
    "    [0.5,  0.67, 0,   0.04, 0.1],\n",
    "    [0.75, 0.2,  0.3, 0.15, 0.4],\n",
    "    [0,    0.3,  0.6, 0.02, 0],\n",
    "    [0.1,  0,  0.2, 0.04, 0.15],\n",
    "    [0.3,    0,    0.4, 0.8,  0.2],\n",
    "    [0.05,  0.7, 0,   0, 0.5],\n",
    "    [0.2, 0,  0.3, 0.8, 0.6],\n",
    "    [0,    0.3,  0.6, 0.1, 0.8],\n",
    "    [0.8,    0.02,  0.3, 0.02, 0],\n",
    "    [0.02,    0.01,  0.6, 0.025, 0.5],\n",
    "    [0.95, 0.8,  0.3, 0.8, 0.8],\n",
    "    [0.08,    0,  0.6, 0.01, 0.9],\n",
    "    [0.02,    0,  0.35, 0.65, 0.5],\n",
    "    [0.5,    0.1,  0.65, 0.1, 0.25],\n",
    "    [0.01,    0.5,  0.55, 0.3, 0.15]  \n",
    "])\n",
    "\n",
    "cov = np.cov(stock_yields.T)\n",
    "\n",
    "print(f\"\"\"Using numpy covariance\n",
    "    cov shape: {cov.shape}\n",
    "    cov[1]: {cov[0]}\n",
    "    cov[n]: {cov[-1]}\n",
    "\"\"\")\n",
    "\n",
    "stock_yields_mean = stock_yields - stock_yields.mean(axis=0)\n",
    "\n",
    "U, S, VT = svd(stock_yields_mean, full_matrices=0)\n",
    "\n",
    "cov = np.dot(np.dot(VT.T, np.diag(S**2)), VT)\n",
    "cov = cov / (stock_yields.shape[0] - 1)\n",
    "\n",
    "print(f\"\"\"Using SVD covariance\n",
    "    cov shape: {cov.shape}\n",
    "    cov[1]: {cov[0]}\n",
    "    cov[n]: {cov[-1]}\n",
    "\"\"\")\n",
    "\n",
    "print('Displaying stocks and their std. covariance\\n', S)\n",
    "\n",
    "print('\\nDisplaying stocks covariance matrix\\n', cov)\n",
    "\n",
    "print('\\nSelecting best stocks with higher combination yields\\n', S[0:2], stocks[0:2])\n",
    "\n",
    "print('\\nGives us related covariance vectors (relationship amongst variables)\\n', cov[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
