{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('wine.csv')\n",
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Center to mean\n",
    "x = x - x.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Getting a sense of SVD\n",
    "\n",
    "To calculate the SVD, imagine having matrix `A`,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*4DncmDEnF9SIYTTrDR0Adw.png\" width=\"650\" height=\"auto\" />\n",
    "\n",
    "Calculate the following eigenvalues and vectors (normalized) from `A^T ⋅ A` and `A ⋅ A^T`,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*RG2fiVNyPDr77eS4qsAjLg.jpeg\" width=\"750\" height=\"auto\" />\n",
    "\n",
    "These will be used to form the matrices for `U`, `S` and `V^T`.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xnlAa8E-c63HnMcTRcb5HA.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))\n",
    "\n",
    "Credits: [Jason Brownlee](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix (m × n) = (178, 13)\n",
      "U matrix (m × n) = (178, 178)\n",
      "S matrix (m × n) = (13,)\n",
      "V^T matrix (m × n) = (13, 13)\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "# A = U * S * V^T\n",
    "A = x\n",
    "U, S, VT = svd(A)\n",
    "\n",
    "print(f'A matrix (m × n) = {A.shape}')\n",
    "print(f'U matrix (m × n) = {U.shape}')\n",
    "print(f'S matrix (m × n) = {S.shape}')\n",
    "print(f'V^T matrix (m × n) = {VT.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Reformulating SVD\n",
    "\n",
    "Reformulating it, we can represent it as such,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*0LEG-KOZYYYsaXnQxMCkXA.gif\" width=\"500\" height=\"auto\" />\n",
    "\n",
    "And each vectors can be formulated as,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*KGcqnL20ihPN4RDyLhQzXA.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "Giving this linear combination,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*0n2-o06c_j42d0MJo7igYQ.gif\" width=\"500\" height=\"auto\" />\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstructed matrix B,\n",
      "\n",
      "A(1) = [ 1.22938202e+00 -6.26348315e-01  6.34831461e-02 -3.89494382e+00\n",
      "  2.72584270e+01  5.04887640e-01  1.03073034e+00 -8.18539326e-02\n",
      "  6.99101124e-01  5.81910118e-01  8.25505618e-02  1.30831461e+00\n",
      "  3.18106742e+02]\n",
      "\n",
      "B(1) = [ 1.22938202e+00 -6.26348315e-01  6.34831461e-02 -3.89494382e+00\n",
      "  2.72584270e+01  5.04887640e-01  1.03073034e+00 -8.18539326e-02\n",
      "  6.99101124e-01  5.81910118e-01  8.25505618e-02  1.30831461e+00\n",
      "  3.18106742e+02]\n",
      "\n",
      "A(n) = [   1.12938202    1.76365169    0.37348315    5.00505618   -3.74157303\n",
      "   -0.24511236   -1.26926966    0.19814607   -0.24089888    4.14191012\n",
      "   -0.34744944   -1.01168539 -186.89325843]\n",
      "\n",
      "B(n) = [   1.12938202    1.76365169    0.37348315    5.00505618   -3.74157303\n",
      "   -0.24511236   -1.26926966    0.19814607   -0.24089888    4.14191012\n",
      "   -0.34744944   -1.01168539 -186.89325843]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "# Reconstruct the matrix and gives back the same, since the last element is 0\n",
    "# This calculates the same way as 2a.\n",
    "# Convert U(m × m) . sigma(n × n) . V^T(n × n)\n",
    "# Into    U(m × m) . sigma(m × n) . V^T(n × n)\n",
    "# Create the m × n matrix\n",
    "sigma = np.zeros(A.shape)\n",
    "# Populate the n × n matrix\n",
    "sigma[:A.shape[1], :A.shape[1]] = np.diag(S)\n",
    "# Reconstruct matrix\n",
    "B = U.dot(sigma.dot(VT))\n",
    "print('\\nReconstructed matrix B,\\n')\n",
    "print(f'A(1) = {A[0]}\\n')\n",
    "print(f'B(1) = {B[0]}\\n')\n",
    "print(f'A(n) = {A[-1]}\\n')\n",
    "print(f'B(n) = {B[-1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Pseudo-Inverse SVD\n",
    "\n",
    "For a linear equation system, compute the inverse of a square matrix `A` to solve `x` where,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*a1inq-_XL9WHTCsxzHpamQ.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "But as not all matrices are invertible, it will be unlikely to find an exact solution. Hence to find the best-fit solution, compute the following pseudoinverse `A+` which minimizes the least square error,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*aQ2MqySUZnIbCIrxfH1G8Q.png\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "Which solution for `x` can be estimated as,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*lF1z-LodZHA3834kseswYw.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "Leading to,\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*ClzObIIjZyQDb9svX4FjjQ.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*mu9Z_NVYd_3CXwhbERVB8Q.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "For illustration purposes (see how the `D+` is constructed for none invertible matrices),\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*xxatolWVNPjMCUEEWLfvyg.jpeg\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix (m × n) = (178, 13)\n",
      "A+ matrix (m × n) = (13, 178)\n",
      "\n",
      "Reconstructed matrix A+,\n",
      "\n",
      "A(1) = [ 1.22938202e+00 -6.26348315e-01  6.34831461e-02 -3.89494382e+00\n",
      "  2.72584270e+01  5.04887640e-01  1.03073034e+00 -8.18539326e-02\n",
      "  6.99101124e-01  5.81910118e-01  8.25505618e-02  1.30831461e+00\n",
      "  3.18106742e+02]\n",
      "\n",
      "A+(1) = [ 1.18824971e-02 -3.65460602e-03  2.11654843e-03 -1.96932056e-03\n",
      "  8.76744001e-04 -1.62499208e-02 -5.95520631e-03  5.27644561e-02\n",
      "  7.05563663e-03  1.50109879e-03 -1.84029302e-02  3.53394217e-02\n",
      " -2.69207210e-05]\n",
      "\n",
      "A(n) = [   1.12938202    1.76365169    0.37348315    5.00505618   -3.74157303\n",
      "   -0.24511236   -1.26926966    0.19814607   -0.24089888    4.14191012\n",
      "   -0.34744944   -1.01168539 -186.89325843]\n",
      "\n",
      "A+(n) = [ 1.72162774e-02  1.30842923e-03  1.67310309e-02  1.23266371e-03\n",
      " -1.83457782e-04  2.03152471e-02 -1.38321782e-02  2.50984340e-02\n",
      "  7.13578657e-03  1.08775708e-03  6.88656512e-03 -3.23671167e-03\n",
      " -3.36088796e-05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "# B = A+\n",
    "A_PI = np.linalg.pinv(A)\n",
    "\n",
    "print(f'A matrix (m × n) = {A.shape}')\n",
    "print(f'A+ matrix (m × n) = {A_PI.shape}')\n",
    "print('\\nReconstructed matrix A+,\\n')\n",
    "print(f'A(1) = {A[0]}\\n')\n",
    "print(f'A+(1) = {A_PI[:,0].flatten()}\\n')\n",
    "print(f'A(n) = {A[-1]}\\n')\n",
    "print(f'A+(n) = {A_PI[:,-1].flatten()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix (m × n) = (178, 13)\n",
      "A+ matrix (m × n) = (13, 178)\n"
     ]
    }
   ],
   "source": [
    "# This calculates the same way as 2c.\n",
    "# Populate D with n × n diagonal matrix based on A's m × n\n",
    "D = np.zeros(A.shape)\n",
    "D[:A.shape[1], :A.shape[1]] = np.diag(1.0 / S)\n",
    "# Calculate pseudoinverse\n",
    "A_PI = VT.T.dot(D.T).dot(U.T)\n",
    "\n",
    "print(f'A matrix (m × n) = {A.shape}')\n",
    "print(f'A+ matrix (m × n) = {A_PI.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction\n",
    "\n",
    "[Read here on \"difference between SVD and PCA\"](https://stats.stackexchange.com/questions/239481/difference-between-scikit-learn-implementations-of-pca-and-truncatedsvd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T matrix (m × n) = (178, 2)\n",
      "\n",
      "T(1) = [318.56297929  21.49213073]\n",
      "\n",
      "T(n) = [-186.94319027   -0.2133308 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(A)\n",
    "\n",
    "T = svd.transform(A)\n",
    "print(f'T matrix (m × n) = {T.shape}')\n",
    "print(f'\\nT(1) = {T[0]}\\n')\n",
    "print(f'T(n) = {T[-1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V^T matrix (m × n) = (13, 13)\n",
      "V^Tk matrix (m × n) = (2, 13)\n",
      "S matrix (m × n) = (178, 13)\n",
      "Sk matrix (m × n) = (178, 2)\n",
      "\n",
      "A matrix (m × n) = (178, 13)\n",
      "B matrix (m × n) = (178, 13)\n",
      "\n",
      "Reconstructed matrix B,\n",
      "\n",
      "A(1) = [ 1.22938202e+00 -6.26348315e-01  6.34831461e-02 -3.89494382e+00\n",
      "  2.72584270e+01  5.04887640e-01  1.03073034e+00 -8.18539326e-02\n",
      "  6.99101124e-01  5.81910118e-01  8.25505618e-02  1.30831461e+00\n",
      "  3.18106742e+02]\n",
      "\n",
      "Bk(1) = [ 5.54444075e-01 -1.70631193e-01  1.60817995e-01 -9.19628125e-01\n",
      "  2.71701216e+01  3.34192369e-01  4.98165648e-01 -6.83214917e-02\n",
      "  2.98886635e-01  1.06588043e+00  3.82038649e-02  1.49442298e-01\n",
      "  3.18124576e+02]\n",
      "\n",
      "A(n) = [   1.12938202    1.76365169    0.37348315    5.00505618   -3.74157303\n",
      "   -0.24511236   -1.26926966    0.19814607   -0.24089888    4.14191012\n",
      "   -0.34744944   -1.01168539 -186.89325843]\n",
      "\n",
      "Bk(n) = [-3.10444964e-01  1.26851497e-01 -3.74162773e-02  8.67625150e-01\n",
      " -3.55349322e+00 -1.85229215e-01 -2.92982814e-01  2.32991653e-02\n",
      " -1.13347129e-01 -4.38264943e-01 -3.18756292e-02 -1.31036502e-01\n",
      " -1.86906298e+02]\n",
      "\n",
      "T = U . Sk (m × n):  (178, 2)\n",
      "T(1) = [-318.56297929  -21.49213073]\n",
      "T(n) = [186.94319027   0.2133308 ]\n",
      "\n",
      "T = A . V^Tk (m × n):  (178, 2)\n",
      "T(1) = [-318.56297929  -21.49213073]\n",
      "T(n) = [186.94319027   0.2133308 ]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# T = V^k . A\n",
    "# T = U . Sigma^k\n",
    "components_n = 2\n",
    "\n",
    "sigma_k = sigma[:, :components_n]\n",
    "VT_k = VT[:components_n, :]\n",
    "\n",
    "print(f'V^T matrix (m × n) = {VT.shape}')\n",
    "print(f'V^Tk matrix (m × n) = {VT_k.shape}')\n",
    "print(f'S matrix (m × n) = {sigma.shape}')\n",
    "print(f'Sk matrix (m × n) = {sigma_k.shape}')\n",
    "\n",
    "# Reconstruct\n",
    "B_k = U.dot(sigma_k.dot(VT_k))\n",
    "\n",
    "print(f'\\nA matrix (m × n) = {A.shape}')\n",
    "print(f'B matrix (m × n) = {B_k.shape}')\n",
    "print('\\nReconstructed matrix B,\\n')\n",
    "print(f'A(1) = {A[0]}\\n')\n",
    "print(f'Bk(1) = {B_k[0]}\\n')\n",
    "print(f'A(n) = {A[-1]}\\n')\n",
    "print(f'Bk(n) = {B_k[-1]}\\n')\n",
    "\n",
    "T_k = U.dot(sigma_k)\n",
    "print('T = U . Sk (m × n): ', T_k.shape)\n",
    "print(f'T(1) = {T_k[0]}')\n",
    "print(f'T(n) = {T_k[-1]}\\n')\n",
    "\n",
    "T_k = A.dot(VT_k.T)\n",
    "print('T = A . V^Tk (m × n): ', T_k.shape)\n",
    "print(f'T(1) = {T_k[0]}')\n",
    "print(f'T(n) = {T_k[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding the top features\n",
    "\n",
    "Note that you can also compress images using SVD - [read here](https://rdsquare.github.io/Image-Compression-SVD-Python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting best 5 features\n",
      " [4190.31224906  174.75337527   40.8723149    29.72269526   14.74807124] ['Alcohol', 'Malic_Acid', 'Ash', 'Ash_Alcanity', 'Magnesium'] \n",
      "\n",
      "Gives us related covariance vectors (relationship amongst variables)\n",
      " [[ 6.59062328e-01  8.56113090e-02  4.71151590e-02 -8.41092903e-01\n",
      "   3.13987812e+00  1.46887218e-01  1.92033222e-01 -1.57542595e-02\n",
      "   6.35175205e-02  1.02828254e+00 -1.33134432e-02  4.16978226e-02\n",
      "   1.64567185e+02]\n",
      " [ 8.56113090e-02  1.24801540e+00  5.02770393e-02  1.07633171e+00\n",
      "  -8.70779534e-01 -2.34337723e-01 -4.58630366e-01  4.07333619e-02\n",
      "  -1.41146982e-01  6.44838183e-01 -1.43325638e-01 -2.92447483e-01\n",
      "  -6.75488666e+01]\n",
      " [ 4.71151590e-02  5.02770393e-02  7.52646353e-02  4.06208278e-01\n",
      "   1.12293658e+00  2.21455913e-02  3.15347299e-02  6.35847140e-03\n",
      "   1.51557799e-03  1.64654327e-01 -4.68215451e-03  7.61835841e-04\n",
      "   1.93197391e+01]\n",
      " [-8.41092903e-01  1.07633171e+00  4.06208278e-01  1.11526862e+01\n",
      "  -3.97476036e+00 -6.71149146e-01 -1.17208281e+00  1.50421856e-01\n",
      "  -3.77176220e-01  1.45024186e-01 -2.09118054e-01 -6.56234368e-01\n",
      "  -4.63355345e+02]\n",
      " [ 3.13987812e+00 -8.70779534e-01  1.12293658e+00 -3.97476036e+00\n",
      "   2.03989335e+02  1.91646988e+00  2.79308703e+00 -4.55563385e-01\n",
      "   1.93283248e+00  6.62052061e+00  1.80851266e-01  6.69308068e-01\n",
      "   1.76915870e+03]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import svd, eigvals\n",
    "\n",
    "# Get the eigenvalues\n",
    "ev = np.sqrt(eigvals(np.dot(x.T, x)))\n",
    "ev_map = list(map(lambda v: v[0], sorted(enumerate(ev), key=lambda v: v[1], reverse=True)))\n",
    "\n",
    "U, S, VT = svd(x, full_matrices=0)\n",
    "\n",
    "cov = np.dot(np.dot(VT.T, np.diag(S**2)), VT)\n",
    "cov = cov / (x.shape[0] - 1)\n",
    "\n",
    "features_n = 5\n",
    "\n",
    "print('Selecting best 5 features\\n', S[0:features_n], [dataset.columns[ev_map[i]] for i in range(features_n)], '\\n')\n",
    "\n",
    "print('Gives us related covariance vectors (relationship amongst variables)\\n', cov[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Another Sample Example\n",
    "\n",
    "For example, we have a matrix contains the return of stock yields traded by different investors.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*FJXUrl22HERjCUe2dR42mA.gif\" width=\"600\" height=\"auto\" />\n",
    "\n",
    "As a fund manager, you want to identify the combination of stocks and investors that have the largest yields.\n",
    "\n",
    "(Source: [Jonathan Hui, 2019](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491))\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/8JGdq.png\" width=\"200\" height=\"auto\" />\n",
    "\n",
    "(Source: [@jeffery_the_wind, 2013](https://math.stackexchange.com/questions/479918/trying-to-check-cov-matrix-calculation-from-svd-using-numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using numpy covariance\n",
      "    cov shape: (5, 5)\n",
      "    cov[1]: [ 0.13167794  0.02458787 -0.03080147  0.01061158 -0.00620037]\n",
      "    cov[n]: [-0.00620037  0.00696324  0.01202206  0.01818566  0.07878676]\n",
      "\n",
      "Using SVD covariance\n",
      "    cov shape: (5, 5)\n",
      "    cov[1]: [ 0.13167794  0.02458787 -0.03080147  0.01061158 -0.00620037]\n",
      "    cov[n]: [-0.00620037  0.00696324  0.01202206  0.01818566  0.07878676]\n",
      "\n",
      "Displaying stocks and their std. covariance\n",
      " [1.57553585 1.39921259 1.14559555 0.9894796  0.67449509] ['GOOG', 'APPL', 'TESLA', 'FB', 'AMZN']\n",
      "\n",
      "Displaying stocks covariance matrix\n",
      " [[ 0.13167794  0.02458787 -0.03080147  0.01061158 -0.00620037]\n",
      " [ 0.02458787  0.07812426 -0.02002206 -0.00925129  0.00696324]\n",
      " [-0.03080147 -0.02002206  0.04652574 -0.00608548  0.01202206]\n",
      " [ 0.01061158 -0.00925129 -0.00608548  0.11404228  0.01818566]\n",
      " [-0.00620037  0.00696324  0.01202206  0.01818566  0.07878676]]\n",
      "\n",
      "Selecting best stocks with higher combination yields\n",
      " [1.57553585 1.39921259] ['GOOG', 'APPL']\n",
      "\n",
      "Gives us related covariance vectors (relationship amongst variables)\n",
      " [[ 0.13167794  0.02458787]\n",
      " [ 0.02458787  0.07812426]\n",
      " [-0.03080147 -0.02002206]\n",
      " [ 0.01061158 -0.00925129]\n",
      " [-0.00620037  0.00696324]]\n"
     ]
    }
   ],
   "source": [
    "stocks = ['GOOG', 'AMZN', 'FB', 'TESLA', 'APPL']\n",
    "stock_yields = np.array([\n",
    "    [0.8,  0.2,  0.1, 0.04, 0.3],\n",
    "    [0,    0,    0.2, 0.8,  0.25],\n",
    "    [0.75,  0.67, 0,   0.04, 0.1],\n",
    "    [0.75, 0.2,  0.3, 0.15, 0.4],\n",
    "    [0,    0.3,  0.6, 0.02, 0],\n",
    "    [0.1,  0,  0.2, 0.04, 0.15],\n",
    "    [0.3,    0,    0.4, 0.8,  0.2],\n",
    "    [0.05,  0.7, 0,   0, 0.5],\n",
    "    [0.62, 0,  0.3, 0.8, 0.6],\n",
    "    [0,    0.3,  0.6, 0.1, 0.8],\n",
    "    [0.8,    0.02,  0.3, 0.02, 0],\n",
    "    [0.02,    0.01,  0.6, 0.025, 0.5],\n",
    "    [0.95, 0.8,  0.3, 0.8, 0.8],\n",
    "    [0.08,    0,  0.6, 0.01, 0.9],\n",
    "    [0.02,    0,  0.35, 0.65, 0.5],\n",
    "    [0.5,    0.1,  0.65, 0.1, 0.25],\n",
    "    [0.01,    0.5,  0.55, 0.3, 0.15]  \n",
    "])\n",
    "\n",
    "cov = np.cov(stock_yields.T)\n",
    "\n",
    "print(f\"\"\"Using numpy covariance\n",
    "    cov shape: {cov.shape}\n",
    "    cov[1]: {cov[0]}\n",
    "    cov[n]: {cov[-1]}\n",
    "\"\"\")\n",
    "\n",
    "stock_yields_mean = stock_yields - stock_yields.mean(axis=0)\n",
    "\n",
    "U, S, VT = svd(stock_yields_mean, full_matrices=0)\n",
    "\n",
    "cov = np.dot(np.dot(VT.T, np.diag(S**2)), VT)\n",
    "cov = cov / (stock_yields.shape[0] - 1)\n",
    "\n",
    "print(f\"\"\"Using SVD covariance\n",
    "    cov shape: {cov.shape}\n",
    "    cov[1]: {cov[0]}\n",
    "    cov[n]: {cov[-1]}\n",
    "\"\"\")\n",
    "\n",
    "# Get eigenvalues\n",
    "ev = np.sqrt(eigvals(np.dot(stock_yields_mean.T, stock_yields_mean)))\n",
    "ev_map = list(map(lambda v: v[0], sorted(enumerate(ev), key=lambda v: v[1], reverse=True)))\n",
    "features_n = 2\n",
    "\n",
    "print('Displaying stocks and their std. covariance\\n', S, [stocks[ev_map[i]] for i in range(len(stocks))])\n",
    "\n",
    "print('\\nDisplaying stocks covariance matrix\\n', cov)\n",
    "\n",
    "print('\\nSelecting best stocks with higher combination yields\\n', S[0:features_n], [stocks[ev_map[i]] for i in range(features_n)])\n",
    "\n",
    "print('\\nGives us related covariance vectors (relationship amongst variables)\\n', cov[:,0:features_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
